"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[888],{784:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>o});var s=t(5893),a=t(1151);const r={},i=void 0,l={id:"Dataset/ReadyToTrain_DS",title:"ReadyToTrain_DS",description:"Brief description of the submodule",source:"@site/docs/Dataset/ReadyToTrain_DS.md",sourceDirName:"Dataset",slug:"/Dataset/ReadyToTrain_DS",permalink:"/CashewDA-docs/docs/Dataset/ReadyToTrain_DS",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Dataset/ReadyToTrain_DS.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DatasetSplit",permalink:"/CashewDA-docs/docs/Dataset/DatasetSplit"},next:{title:"Transforms",permalink:"/CashewDA-docs/docs/Dataset/Transforms"}},d={},o=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"calculate_percentiles()",id:"calculate_percentiles",level:2},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"get_LOVE_DataLoaders()",id:"get_love_dataloaders",level:2},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"Source code",id:"source-code-1",level:3}];function c(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,s.jsx)(n.p,{children:"Here all of the code used to pass from the datasets downloaded to ready to use data for training is described."}),"\n",(0,s.jsx)(n.h2,{id:"calculate_percentiles",children:"calculate_percentiles()"}),"\n",(0,s.jsx)(n.p,{children:"Function to calculate 0.01 and 0.99 percentiles of the bands of planet images. These values will be later used for normalizing the dataset."}),"\n",(0,s.jsx)(n.h3,{id:"params",children:"Params"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"img_folder:"})," ",(0,s.jsx)(n.em,{children:"(list)"})," The name of the folder with the images."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"samples:"})," ",(0,s.jsx)(n.em,{children:"(integer)"})," The number of images to take to calculate these percentiles, for computing reasons not all images are considered."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"outputs",children:"Outputs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vals:"})," ",(0,s.jsx)(n.em,{children:"(numpy.ndarray)"})," The mean 1% and 99% quantiles for the images analysed."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"os"}),"\n",(0,s.jsx)(n.li,{children:"random"}),"\n",(0,s.jsx)(n.li,{children:"numpy"}),"\n",(0,s.jsx)(n.li,{children:"rioxarray"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-code",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    imgs = [fn for fn in os.listdir(img_folder) if 'StudyArea' in fn]\n\n    random.seed(8)\n    img_sample = random.sample(imgs, samples)\n    quantiles = np.zeros((2,4))\n    \n    for i in img_sample:\n        quantiles += rioxarray.open_rasterio(img_folder + \"\\\\\" + i).quantile((0.01, 0.99), dim = ('x','y')).values\n    \n    vals = quantiles/len(img_sample)\n    \n    return vals\n"})}),"\n",(0,s.jsx)(n.h2,{id:"get_love_dataloaders",children:"get_LOVE_DataLoaders()"}),"\n",(0,s.jsx)(n.p,{children:"Function to get the loaders for LoveDA dataset."}),"\n",(0,s.jsx)(n.p,{children:"Size of the dataset:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Domain"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Train"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Validation"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Test"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Urban"})}),(0,s.jsx)(n.td,{children:"1,155"}),(0,s.jsx)(n.td,{children:"677"}),(0,s.jsx)(n.td,{children:"820"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Rural"})}),(0,s.jsx)(n.td,{children:"1,366"}),(0,s.jsx)(n.td,{children:"992"}),(0,s.jsx)(n.td,{children:"976"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"params-1",children:"Params"}),"\n",(0,s.jsx)(n.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,s.jsx)(n.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"torchgeo.datasets -> LoveDA"}),"\n",(0,s.jsx)(n.li,{children:"torch"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def get_DataLoaders(dir, batch_size, transform, normalization, VI, only_get_DS = False, train_split_size = None, val_split_size = None):\n    \"\"\"\n        Function to get the loaders for LoveDA dataset.\n\n        Inputs:\n            - domain: List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'.\n            - batch_size: Number of images per batch.\n            - transforms: Image augmentations that will be considered.\n            - train_split_size: Amount of images from training split that will be considered. (Float between 0 and 1)\n            - val_split_size: Amount of images from validation split that will be considered. (Float between 0 and 1)\n            - only_get_DS: Boolean for only getting datasets instead of dataloaders.\n\n        Output:\n            - train_loader: Training torch LoveDA data loader\n            - val_loader: Validation torch LoveDA data loader\n            - test_loader: Test torch LoveDA data loader\n    \"\"\"\n\n    if transforms != None:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True, transforms = transforms)\n    else:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True)\n        \n    test_DS = LoveDA('LoveDA', split = 'test', scene = domain, download = True)\n    val_DS = LoveDA('LoveDA', split = 'val', scene = domain, download = True)\n\n    if train_split_size != None:\n        if val_split_size == None:\n            val_split_size = train_split_size\n        train_DS, l = random_split(train_DS, [train_split_size, 1-train_split_size], generator=torch.Generator().manual_seed(8))\n        val_DS, l = random_split(val_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        test_DS, l = random_split(test_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_DS, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_DS, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(dataset=test_DS, batch_size=batch_size, shuffle=False)\n\n    if only_get_DS:\n        return train_DS, val_DS, test_DS\n    else:\n        return train_loader, val_loader, test_loader\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>i});var s=t(7294);const a={},r=s.createContext(a);function i(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);