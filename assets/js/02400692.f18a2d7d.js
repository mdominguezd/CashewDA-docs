"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[762],{3351:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var i=t(5893),a=t(1151);const s={},o=void 0,r={id:"Training/Train_DomainOnly",title:"Train_DomainOnly",description:"Brief description of the submodule",source:"@site/docs/Training/Train_DomainOnly.md",sourceDirName:"Training",slug:"/Training/Train_DomainOnly",permalink:"/CashewDA-docs/docs/Training/Train_DomainOnly",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Training/Train_DomainOnly.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Train_DANN",permalink:"/CashewDA-docs/docs/Training/Train_DANN"},next:{title:"Validate",permalink:"/CashewDA-docs/docs/category/validate"}},l={},d=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"evaluate()",id:"evaluate",level:2},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"training_loop()",id:"training_loop",level:2},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"train_3fold_DomainOnly()",id:"train_3fold_domainonly",level:2},{value:"train_LoveDA_DomainOnly()",id:"train_loveda_domainonly",level:2}];function c(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,i.jsx)(n.p,{children:"In this submodule, all of the functions used for training the domain-only models are described in detail."}),"\n",(0,i.jsx)(n.h2,{id:"evaluate",children:"evaluate()"}),"\n",(0,i.jsx)(n.p,{children:"Function used to evaluate the segmentation performance of a specified network. It gets the predictions calculated using the network for a specified data loader and calculates the mean loss and accuracy comparing the predictions with the ground truth labels of the loader."}),"\n",(0,i.jsx)(n.h3,{id:"params",children:"Params"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"net:"})," (torch.nn.Module) Network class used to get the segmentation predictions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_loader:"}),"  (torch.nn.DataLoader) Data loader of which the mean loss and accuracy will be calculated."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"loss_function:"})," (torch.nn.Module) Loss function used during the training of the network."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"accu_function:"})," (torch.nn.Module==?==) Function to calculate the mean accuracy of the network on validate_loader. ==Default== is BinaryF1Score()"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"outputs",children:"Outputs"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"metric:"})," (list) List with the mean values of accuracy and loss."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torchmetrics.classification import BinaryF1Score\n\nfrom utils import LOVE_resample_fly\n"})}),"\n",(0,i.jsx)(n.h3,{id:"source-code",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\ndef evaluate(net, validate_loader, loss_function, accu_function = BinaryF1Score(), Love = False, binary_love = False):\n    """\n        Function to evaluate the performance of a network on a validation data loader.\n\n        Inputs:\n            - net: Pytorch network that will be evaluated.\n            - validate_loader: Validation (or Test) dataset with which the network will be evaluated.\n            - loss_function: Loss function used to evaluate the network.\n            - accu_function: Accuracy function used to evaluate the network.\n\n        Output:\n            - metric: List with loss and accuracy values calculated for the validation/test dataset.\n    """\n    \n    net.eval()  # Set the model to evaluation mode\n    device = next(iter(net.parameters())).device # Get training device ("cuda" or "cpu")\n\n    f1_scores = []\n    losses = []\n\n    with torch.no_grad():\n        # Iterate over validate loader to get mean accuracy and mean loss\n        for i, Data in enumerate(validate_loader):\n            \n            # The inputs and GT are obtained differently depending of the Dataset (LoveDA or our own DS)\n            if Love:\n                inputs = LOVE_resample_fly(Data[\'image\'])\n                GTs = LOVE_resample_fly(Data[\'mask\'])\n                if binary_love:\n                    GTs = (GTs == 6).long()\n            else:\n                inputs = Data[0]\n                GTs = Data[1]\n        \n\n            inputs = inputs.to(device)\n            GTs = GTs.type(torch.long).squeeze().to(device)\n            pred = net(inputs)\n        \n            f1 = accu_function.to(device)\n        \n            if (pred.max(1)[1].shape != GTs.shape):\n                GTs = GTs[None, :, :]\n\n            loss = loss_function(pred, GTs)/GTs.shape[0]\n        \n            f1_score = f1(pred.max(1)[1], GTs)\n            \n            f1_scores.append(f1_score.to(\'cpu\').numpy())\n            losses.append(loss.to(\'cpu\').numpy())\n\n        metric = [np.mean(f1_scores), np.mean(losses)]   \n        \n    return metric\n'})}),"\n",(0,i.jsx)(n.h2,{id:"training_loop",children:"training_loop()"}),"\n",(0,i.jsx)(n.p,{children:"Function to train the neural network through backward propagation."}),"\n",(0,i.jsx)(n.h3,{id:"params-1",children:"Params"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"train_loader:"})," DataLoader with the training dataset."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"val_loader:"})," DataLoader with the validation dataset."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"learning_rate:"})," Initial learning rate for training the network."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"starter_channels:"})," Starting number of channels in th U-Net"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"momentum:"})," Momentum used during training."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"number_epochs:"})," Number of training epochs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"loss_function:"})," Function to calculate loss."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"accu_function:"})," Function to calculate accuracy (Default: BinaryF1Score)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Love:"})," Boolean to decide between training with LoveDA dataset or our own dataset."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"decay:"})," Factor in which learning rate decays."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"bilinear:"})," Boolean to decide the upscaling method (If True Bilinear if False Transpose convolution. Default: True)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"n_channels:"})," Number of initial channels (Defalut 4 [Planet])"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"n_classes:"})," Number of classes that will be predicted (Default 2 [Binary segmentation])"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"plot:"})," Boolean to decide if training loop should be plotted or not."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"seed:"})," Seed that will be used for generation of random values."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,i.jsx)(n.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,i.jsx)(n.h2,{id:"train_3fold_domainonly",children:"train_3fold_DomainOnly()"}),"\n",(0,i.jsx)(n.h2,{id:"train_loveda_domainonly",children:"train_LoveDA_DomainOnly()"})]})}function u(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>o});var i=t(7294);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);