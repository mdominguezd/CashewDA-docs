"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[479],{7798:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var a=n(5893),t=n(1151);const i={},r=void 0,l={id:"Training/Train_DANN",title:"Train_DANN",description:"Brief description of the submodule",source:"@site/docs/Training/Train_DANN.md",sourceDirName:"Training",slug:"/Training/Train_DANN",permalink:"/CashewDA-docs/docs/Training/Train_DANN",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Training/Train_DANN.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Hyper_Parameter_Tuning",permalink:"/CashewDA-docs/docs/Training/Hyper_Parameter_Tuning"},next:{title:"Train_DomainOnly",permalink:"/CashewDA-docs/docs/Training/Train_DomainOnly"}},o={},c=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"Last meeting",id:"last-meeting",level:2},{value:"initialize_Unet_DANN()",id:"initialize_unet_dann",level:2},{value:"Number of parameters",id:"number-of-parameters",level:3},{value:"For LoveDA using:",id:"for-loveda-using",level:4},{value:"Source code",id:"source-code",level:3},{value:"evaluate()",id:"evaluate",level:2},{value:"Source code",id:"source-code-1",level:3},{value:"evaluate_disc()",id:"evaluate_disc",level:2},{value:"Source code",id:"source-code-2",level:3},{value:"DANN_training_loop()",id:"dann_training_loop",level:2},{value:"Detailed description",id:"detailed-description",level:3},{value:"Get the source and target datasets",id:"get-the-source-and-target-datasets",level:4},{value:"Initialization of Network to be trained and optimizer",id:"initialization-of-network-to-be-trained-and-optimizer",level:4},{value:"Start training loop",id:"start-training-loop",level:4},{value:"Overview of all loops",id:"overview-of-all-loops",level:5},{value:"Loop per epoch",id:"loop-per-epoch",level:6},{value:"Loop over number of batches",id:"loop-over-number-of-batches",level:6},{value:"Params",id:"params",level:3},{value:"New important inclusion",id:"new-important-inclusion",level:4},{value:"Outputs",id:"outputs",level:3},{value:"Source code (Simpler version [Only shown for LoveDA])",id:"source-code-simpler-version-only-shown-for-loveda",level:3},{value:"Preliminary results",id:"preliminary-results",level:2},{value:"Training loop with 50% LoveDA dataset",id:"training-loop-with-50-loveda-dataset",level:4},{value:"Important params",id:"important-params",level:5},{value:"Observed behaviour",id:"observed-behaviour",level:5},{value:"Gradient flow",id:"gradient-flow",level:5},{value:"Example predictions",id:"example-predictions",level:5}];function d(e){const s={a:"a",annotation:"annotation",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",img:"img",input:"input",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,a.jsx)(s.h2,{id:"last-meeting",children:"Last meeting"}),"\n",(0,a.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(s.li,{className:"task-list-item",children:[(0,a.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Perform Hyperparameter tuning for UNET DANN in LoveDA dataset and hope to get better results."]}),"\n",(0,a.jsxs)(s.li,{className:"task-list-item",children:[(0,a.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Implement the possibility of adding a limited amount of target images to increase UNET DANN performance. (",(0,a.jsx)(s.strong,{children:"Semi-supervised"}),")"]}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"initialize_unet_dann",children:"initialize_Unet_DANN()"}),"\n",(0,a.jsx)(s.h3,{id:"number-of-parameters",children:"Number of parameters"}),"\n",(0,a.jsx)(s.h4,{id:"for-loveda-using",children:"For LoveDA using:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.code,{children:"starter_channels"})," = 16"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.code,{children:"attention"})," = True"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.code,{children:"up_layer"})," = 4"]}),"\n"]}),"\n",(0,a.jsxs)(s.table,{children:[(0,a.jsx)(s.thead,{children:(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Part of the model"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"# of parameters"})})]})}),(0,a.jsxs)(s.tbody,{children:[(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:"Feature extractor"}),(0,a.jsx)(s.td,{children:"1'103,524"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:"Classifier"}),(0,a.jsx)(s.td,{children:"136"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:"Discriminator"}),(0,a.jsx)(s.td,{children:"134'546,496"})]})]})]}),"\n",(0,a.jsx)(s.h3,{id:"source-code",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'def initialize_Unet_DANN(n_channels, n_classes, bilinear = True, starter = 16, up_layer = 4, attention = True, Love = False, grad_rev_w = 1):\n    """\n        Function to initialize U-Net and the discriminator that will be trained using UNet-DANN\n\n        Inputs:\n            - n_channels: Number of channels of input images.\n            - n_classes: Number of classes to be segmented on the images.\n            - bilinear: Boolean used for upsamplimg method. (True: Bilinear is used. False: Transpose convolution is used.) [Default = True]\n            - starter: Start number of channels of the UNet. [Default = 16]\n            - up_layer: Upward step layer in which the U_Net is divided into Feature extractor and Classifier. [Default = 4]\n            - attention: Boolean that describes if attention gates in the UNet will be used or not. [Default = True]\n\n        Outputs:\n            - network: U-Net architecture to be trained.\n            - discriminator: Discriminator that will be trained.\n    """\n    device = get_training_device()\n\n    # Calculate the number  of features that go in the fully connected layers of the discriminator\n    if Love:\n        img_size = 1024\n    else:\n        img_size = 256\n        \n    in_feat = (img_size//(2**4))**2 * starter*(2**3) \n    \n    seed = 123\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    network = UNetDANN(n_channels=n_channels, n_classes=n_classes, bilinear = bilinear, starter = starter, up_layer = up_layer, attention = attention, DA = True, in_feat = in_feat, grad_rev_w = grad_rev_w).to(device)\n\n    return network\n'})}),"\n",(0,a.jsx)(s.h2,{id:"evaluate",children:"evaluate()"}),"\n",(0,a.jsx)(s.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"    def evaluate(net, validate_loader, loss_function, accu_function = BinaryF1Score(), Love = False, binary_love = False, revgrad = 1):\n    \"\"\"\n        Function to evaluate the performance of a network on a validation data loader.\n\n        Inputs:\n            - net: Pytorch network that will be evaluated.\n            - validate_loader: Validation (or Test) dataset with which the network will be evaluated.\n            - loss_function: Loss function used to evaluate the network.\n            - accu_function: Accuracy function used to evaluate the network.\n\n        Output:\n            - metric: List with loss and accuracy values calculated for the validation/test dataset.\n    \"\"\"\n    \n    net.eval()  # Set the model to evaluation mode\n    device = get_training_device()\n\n    f1_scores = []\n    losses = []\n\n    with torch.no_grad():\n        # Iterate over validate loader to get mean accuracy and mean loss\n        for i, Data in enumerate(validate_loader):\n            \n            # The inputs and GT are obtained differently depending of the Dataset (LoveDA or our own DS)\n            if Love:\n                inputs = Data['image']\n                GTs = Data['mask']\n                if binary_love:\n                    GTs = (GTs == 6).long()\n                    \n            else:\n                inputs = Data[0]\n                GTs = Data[1]\n        \n            inputs = inputs.to(device)\n            GTs = GTs.type(torch.long).squeeze().to(device)\n            pred = net(inputs, revgrad)[0]\n        \n            f1 = accu_function.to(device)\n        \n            if (pred.max(1)[1].shape != GTs.shape):\n                GTs = GTs[None, :, :]\n\n            loss = loss_function(pred, GTs)/GTs.shape[0]\n            \n            f1_score = f1(pred.max(1)[1], GTs)\n            \n            f1_scores.append(f1_score.to('cpu').numpy())\n            losses.append(loss.to('cpu').numpy())\n\n        metric = [np.nanmean(f1_scores), np.nanmean(losses)]   \n        \n    return metric\n"})}),"\n",(0,a.jsx)(s.h2,{id:"evaluate_disc",children:"evaluate_disc()"}),"\n",(0,a.jsx)(s.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'def evaluate_disc(network, validate_loaders, device, Love = False):\n    """\n        Function to evaluate the performance of the discriminator\n\n        Inputs:\n            - discriminator: Discriminator head used for the adversarial training\n            \n    """\n\n    network.eval()\n\n    OA = []\n\n    k = -1\n    \n    for loader in validate_loaders:\n        \n        k+=1\n        \n        with torch.no_grad():\n            # Iterate over validate loader to get mean accuracy and mean loss\n            for i, Data in enumerate(loader):\n                # The inputs and GT are obtained differently depending of the Dataset (LoveDA or our own DS)\n                if Love:\n                    inputs = Data[\'image\']\n                else:\n                    inputs = Data[0]\n    \n                inputs = inputs.to(device)\n                GTs = k*torch.ones(inputs.shape[0]).to(device)\n\n                features = network.FE(inputs)\n                dom_preds = network.D(features, revgrad)\n\n                preds = dom_preds.detach().cpu().numpy() > 0\n                oa = (preds == GTs.detach().cpu().numpy())\n\n                OA.append(np.mean(oa))\n                \n    OA_metric = np.mean(OA)\n    \n    return OA_metric\n'})}),"\n",(0,a.jsx)(s.h2,{id:"dann_training_loop",children:"DANN_training_loop()"}),"\n",(0,a.jsx)(s.p,{children:"Function to carry out the training loop for UNet-DANN."}),"\n",(0,a.jsx)(s.h3,{id:"detailed-description",children:"Detailed description"}),"\n",(0,a.jsxs)(s.p,{children:["The implementation of DANN was based on the code found ",(0,a.jsx)(s.a,{href:"https://github.com/jvanvugt/pytorch-domain-adaptation/blob/master/revgrad.py",children:"here"}),"."]}),"\n",(0,a.jsx)(s.h4,{id:"get-the-source-and-target-datasets",children:"Get the source and target datasets"}),"\n",(0,a.jsxs)(s.p,{children:["To do so, the ",(0,a.jsx)(s.code,{children:"DS_args"})," parameter is used on function ",(0,a.jsx)(s.a,{href:"../Dataset/ReadyToTrain_DS#get_love_dataloaders",children:"get_LOVE_DataLoaders"}),"."]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:["When the LoveDA dataset is used ",(0,a.jsx)(s.code,{children:"DS_args"})," has at least 3 parameters:"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"batch_size:"})," ",(0,a.jsx)(s.em,{children:"(int)"})," Number of images per batch."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"transforms:"})," ",(0,a.jsx)(s.em,{children:"(torchvision.transforms.v2.Compose)"})," Transformations performed on the dataset."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"only_get_DS:"})," ",(0,a.jsx)(s.em,{children:"(Boolean)"})," Boolean for only getting datasets instead of dataloaders.","\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["This needs to be ",(0,a.jsx)(s.strong,{children:"True"})," for training DANN because the batch_size for training and validation will be different."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"    # Get DataLoaders for LoveDA dataset\n    if Love:\n        source_DS, source_val_DS, s_te = get_LOVE_DataLoaders(source_domain, *DS_args) \n        target_DS, target_val_DS, t_te = get_LOVE_DataLoaders(target_domain, *DS_args)\n        \n        n_channels = source_DSs[0].__getitem__(0)['image'].size()[-3]\n        \n    # Get Batch size\n    batch_size = DS_args[0]\n\n    # Get train loaders\n    source_loader = torch.utils.data.DataLoader(dataset=source_DS, batch_size=batch_size//2, shuffle=True)\n    target_loader = torch.utils.data.DataLoader(dataset=target_DS, batch_size=batch_size//2, shuffle=True)\n\n    # Get validation loaders\n    source_val_loader = torch.utils.data.DataLoader(dataset=source_DSs[1], batch_size=batch_size, shuffle=False)\n    target_val_loader = torch.utils.data.DataLoader(dataset=target_DSs[1], batch_size=batch_size, shuffle=False)\n"})}),"\n",(0,a.jsx)(s.h4,{id:"initialization-of-network-to-be-trained-and-optimizer",children:"Initialization of Network to be trained and optimizer"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"    # Initialize the networks to be trained and the optimizer\n    network = initialize_Unet_DANN(n_channels, *network_args)\n    optim = torch.optim.Adam([{'params': network.FE.parameters(), 'lr': learning_rate_seg},\n                              {'params': network.C.parameters(), 'lr': learning_rate_seg},\n                              {'params': network.D.parameters(), 'lr': learning_rate_disc},\n                              ])\n"})}),"\n",(0,a.jsx)(s.h4,{id:"start-training-loop",children:"Start training loop"}),"\n",(0,a.jsx)(s.h5,{id:"overview-of-all-loops",children:"Overview of all loops"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"\n    # Training loop \n    for epoch in tqdm(range(epochs), desc = 'Training UNet-DANN model'):\n\n        ...\n\n        # Iterate over pair of batches\n        for source, target in tqdm(batches, disable=True, total=n_batches):\n\n            ...\n\n\n"})}),"\n",(0,a.jsx)(s.h6,{id:"loop-per-epoch",children:"Loop per epoch"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"\n    # Training loop \n    for epoch in tqdm(range(epochs), desc = 'Training UNet-DANN model'):\n\n        # Group batches from both domains\n        batches = zip(source_loader, target_loader)\n        \n        # The number of iterations per, epoch will be dependant of the smallest dataset \n        # (((This is what I was trying to avoid with my edition)))\n        n_batches = min(len(source_loader), len(target_loader))\n\n        # Initialize variables for calculating total domain loss, total segmentation accuracy and total segmentation loss\n        total_domain_loss = total_seg_accuracy = total_seg_loss = 0\n\n        # Maximum value of lambda\n        l_max = 0.3\n\n        # Update weight of the gradient reversal layer\n        revgrad = np.max([0, l_max*(epoch - e_0)/(epochs - e_0)])\n\n        # Iterate over pair of batches\n        for source, target in tqdm(batches, disable=True, total=n_batches):\n\n            ...\n\n        dom_loss = total_domain_loss / n_batches\n        segmentation_loss = total_seg_loss / n_batches\n        seg_accuracy = total_seg_accuracy / n_batches\n"})}),"\n",(0,a.jsx)(s.h6,{id:"loop-over-number-of-batches",children:"Loop over number of batches"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"\n        # Iterate over pair of batches\n        for source, target in tqdm(batches, disable=True, total=n_batches):\n\n            if Love:\n                source_img = source['image']\n                source_msk = source['mask']\n                \n                target_img = target['image']\n                target_msk = target['mask']\n\n            # Concatenate images from both domains\n            imgs = torch.cat([source_img, target_img])\n            imgs = imgs.to(device)\n\n            # Create tensor with domain labels for both domains (source = 1, target = 0)\n            domain_gt = torch.cat([torch.ones(source_img.shape[0]),\n                                   torch.zeros(target_img.shape[0])])\n\n            # Send to device (Cuda)\n            domain_gt = domain_gt.to(device)\n            mask_gt = source_msk.to(device)\n\n            # Extract features\n            features = network.FE(imgs)\n            dw = network.FE.DownSteps(imgs)\n\n            # Get predictions\n            seg_preds = network.C(features, dw)\n            dom_preds = network.D(features, revgrad)\n    \n            # Calculate the loss functions\n            segmentation_loss = seg_loss(seg_preds[:source_img.shape[0]], mask_gt)\n            discriminator_loss = domain_loss(dom_preds.squeeze(), domain_gt)\n\n            seg_imp = 1\n            \n            # Total loss\n            loss = seg_imp*segmentation_loss + (2-seg_imp)*discriminator_loss\n\n            # set the gradients of the model to 0 and perform the backward propagation\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            # Accumulate losses and accuracy \n            total_domain_loss += discriminator_loss.item()\n            total_seg_loss += segmentation_loss.item()\n            accu_function = accu_function.to(device)\n            accu = accu_function(seg_preds[:source_img.shape[0]].max(1)[1], mask_gt)\n            total_seg_accuracy += accu.item()\n            \n"})}),"\n",(0,a.jsx)(s.h3,{id:"params",children:"Params"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"source_domain"})," Either str with name of the source domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"target_domain"})," Either str with name of the target domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"DS_args"})," List of arguments for dataset creation."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"network_args"})," List of arguments for neural network creation. (e.g. n_classes, bilinear, starter, up_layer, attention)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"learning_rate_seg"})," Learning rate for segmentation task."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"learning_rate_disc"})," Learning rate for domain classification task."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"momentum"})," Momentum used on optimizer during training."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"epochs"})," Total number of epochs to train the model."]}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"e_0"})}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Love"})," default = False"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"binary_love"})," default = False"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"seg_loss"})," default = torch.nn.CrossEntropyLoss()"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"domain_loss"})," default = torch.nn.BCEWithLogitsLoss()"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"accu_function"})," default = BinaryF1Score()"]}),"\n"]}),"\n",(0,a.jsx)(s.h4,{id:"new-important-inclusion",children:"New important inclusion"}),"\n",(0,a.jsx)(s.span,{className:"katex-display",children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"\u03bb"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mi,{children:"m"}),(0,a.jsx)(s.mi,{children:"a"}),(0,a.jsx)(s.mi,{children:"x"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mo,{fence:"true",children:"("}),(0,a.jsx)(s.mn,{children:"0"}),(0,a.jsx)(s.mo,{separator:"true",children:","}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"\u03bb"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"m"}),(0,a.jsx)(s.mi,{children:"a"}),(0,a.jsx)(s.mi,{children:"x"})]})]}),(0,a.jsx)(s.mo,{children:"\u22c5"}),(0,a.jsxs)(s.mfrac,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"p"}),(0,a.jsx)(s.mi,{children:"o"}),(0,a.jsx)(s.mi,{children:"c"}),(0,a.jsx)(s.mi,{children:"h"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mn,{children:"0"})]})]}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"p"}),(0,a.jsx)(s.mi,{children:"o"}),(0,a.jsx)(s.mi,{children:"c"}),(0,a.jsx)(s.mi,{children:"h"}),(0,a.jsx)(s.mi,{children:"s"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mn,{children:"0"})]})]})]}),(0,a.jsx)(s.mo,{fence:"true",children:")"})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"    \\lambda = max\\left(0, \\lambda_{max} \\cdot \\frac{epoch - e_0}{epochs - e_0}\\right)"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03bb"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"2.4em",verticalAlign:"-0.95em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"ma"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"minner",children:[(0,a.jsx)(s.span,{className:"mopen delimcenter",style:{top:"0em"},children:(0,a.jsx)(s.span,{className:"delimsizing size3",children:"("})}),(0,a.jsx)(s.span,{className:"mord",children:"0"}),(0,a.jsx)(s.span,{className:"mpunct",children:","}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03bb"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1514em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"ma"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"x"})]})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22c5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,a.jsx)(s.span,{className:"mfrac",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"1.3714em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.314em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"oc"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"h"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"0"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})]}),(0,a.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,a.jsxs)(s.span,{style:{top:"-3.677em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"oc"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"h"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"0"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.8804em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose nulldelimiter"})]}),(0,a.jsx)(s.span,{className:"mclose delimcenter",style:{top:"0em"},children:(0,a.jsx)(s.span,{className:"delimsizing size3",children:")"})})]})]})]})]})}),"\n",(0,a.jsx)(s.h3,{id:"outputs",children:"Outputs"}),"\n",(0,a.jsx)(s.h3,{id:"source-code-simpler-version-only-shown-for-loveda",children:"Source code (Simpler version [Only shown for LoveDA])"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"def DANN_training_loop(source_domain, target_domain, DS_args, network_args, learning_rate_seg, learning_rate_disc, epochs, e_0, Love = False, binary_love = False, seg_loss = torch.nn.CrossEntropyLoss(), domain_loss = torch.nn.BCEWithLogitsLoss(), accu_function = BinaryF1Score()):\n    \"\"\"\n        Function to carry out the training of UNet-DANN.\n\n        Inputs:\n            - source_domain: List with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset.\n            - target_domain: List with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset.\n            - DS_args: List of arguments for dataset creation. \n                - For LoveDA: Should at least have: (batch_size, transforms, only_get_DS) {only_get_DS: To get Datasets instead of DataLoaders (Different batchsize in train and validation)}\n            - network_args: List of arguments for neural network creation. (e.g. n_classes, bilinear, starter, up_layer, attention)\n    \"\"\"\n\n    # Get DataLoaders for LoveDA dataset\n    if Love:\n        source_DS, source_val_DS, s_te = get_LOVE_DataLoaders(source_domain, *DS_args) \n        target_DS, target_val_DS, t_te = get_LOVE_DataLoaders(target_domain, *DS_args)\n        \n        n_channels = source_DSs[0].__getitem__(0)['image'].size()[-3]\n\n    batch_size = DS_args[0]\n\n    # Get train loaders\n    source_loader = torch.utils.data.DataLoader(dataset=source_DS, batch_size=batch_size//2, shuffle=True)\n    target_loader = torch.utils.data.DataLoader(dataset=target_DS, batch_size=batch_size//2, shuffle=True)\n\n    # Get validation loaders\n    source_val_loader = torch.utils.data.DataLoader(dataset=source_DSs[1], batch_size=batch_size, shuffle=False)\n    target_val_loader = torch.utils.data.DataLoader(dataset=target_DSs[1], batch_size=batch_size, shuffle=False)\n\n    # Initialize the networks to be trained and the optimizer\n    network = initialize_Unet_DANN(n_channels, *network_args)\n    optim = torch.optim.Adam([{'params': network.FE.parameters(), 'lr': learning_rate_seg},\n                              {'params': network.C.parameters(), 'lr': learning_rate_seg},\n                              {'params': network.D.parameters(), 'lr': learning_rate_disc},\n                              ])\n        \n    # Training loop \n    for epoch in tqdm(range(epochs), desc = 'Training UNet-DANN model'):\n\n        # Group batches from both domains\n        batches = zip(source_loader, target_loader)\n        \n        # The number of iterations per, epoch will be dependant of the smallest dataset (This is what I was trying to avoid with my edition)\n        n_batches = min(len(source_loader), len(target_loader))\n\n        # Initialize variables for calculating total domain loss, total segmentation accuracy and total segmentation loss\n        total_domain_loss = total_seg_accuracy = total_seg_loss = 0\n\n        # Maximum value of lambda\n        l_max = 0.3\n\n        # Update weight of the gradient reversal layer\n        revgrad = np.max([0, l_max*(epoch - e_0)/(epochs - e_0)])\n\n        # Iterate over pair of batches\n        for source, target in tqdm(batches, disable=True, total=n_batches):\n    \n            if Love:\n                source_img = source['image']\n                source_msk = source['mask']\n                \n                target_img = target['image']\n                target_msk = target['mask']\n\n            # Concatenate images from both domains\n            imgs = torch.cat([source_img, target_img])\n            imgs = imgs.to(device)\n\n            # Create tensor with domain labels for both domains (source = 1, target = 0)\n            domain_gt = torch.cat([torch.ones(source_img.shape[0]),\n                                   torch.zeros(target_img.shape[0])])\n\n            # Send to device (Cuda)\n            domain_gt = domain_gt.to(device)\n            mask_gt = source_msk.to(device)\n\n            # Extract features\n            features = network.FE(imgs)\n            dw = network.FE.DownSteps(imgs)\n\n            # Get predictions\n            seg_preds = network.C(features, dw)\n            dom_preds = network.D(features, revgrad)\n    \n            # Calculate the loss functions\n            segmentation_loss = seg_loss(seg_preds[:source_img.shape[0]], mask_gt)\n            discriminator_loss = domain_loss(dom_preds.squeeze(), domain_gt)\n\n            seg_imp = 1\n            \n            # Total loss\n            loss = seg_imp*segmentation_loss + (2-seg_imp)*discriminator_loss\n\n            # set the gradients of the model to 0 and perform the backward propagation\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            # Accumulate losses and accuracy \n            total_domain_loss += discriminator_loss.item()\n            total_seg_loss += segmentation_loss.item()\n            accu_function = accu_function.to(device)\n            accu = accu_function(seg_preds[:source_img.shape[0]].max(1)[1], mask_gt)\n            total_seg_accuracy += accu.item()\n\n        dom_loss = total_domain_loss / n_batches\n        segmentation_loss = total_seg_loss / n_batches\n        seg_accuracy = total_seg_accuracy / n_batches\n"})}),"\n",(0,a.jsx)(s.h2,{id:"preliminary-results",children:"Preliminary results"}),"\n",(0,a.jsx)(s.h4,{id:"training-loop-with-50-loveda-dataset",children:"Training loop with 50% LoveDA dataset"}),"\n",(0,a.jsx)(s.h5,{id:"important-params",children:"Important params"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.code,{children:"e_0"})," = 10"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.code,{children:"learning_rate"})," = 0.001"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"\u03bb"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"m"}),(0,a.jsx)(s.mi,{children:"a"}),(0,a.jsx)(s.mi,{children:"x"})]})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\lambda_{max}"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03bb"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1514em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"ma"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"x"})]})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})})]})})," = 1"]}),"\n"]}),"\n",(0,a.jsx)(s.h5,{id:"observed-behaviour",children:"Observed behaviour"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["The training loop follows expected behaviour according to Brion et al. (2021):","\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["During initial epochs until ",(0,a.jsx)(s.code,{children:"e_0"}),", the test domain classifier loss reaches a theoretical minimum that would represent a perfectly classified domain (1.0)"]}),"\n",(0,a.jsx)(s.li,{children:"During the rest of the training the test domain classifier loss goes up to its theoretical maximum (This loss would be associated with a domain classifer accuracy around 0.5)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"However,"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"During the whole training, segmentation accuracy does not go up to the theoretical maximum. (On this training gets to a max level around 0.13)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"img",src:n(2557).Z+"",width:"700",height:"500"})}),"\n",(0,a.jsx)(s.h5,{id:"gradient-flow",children:"Gradient flow"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"img",src:n(9885).Z+"",width:"1476",height:"886"})}),"\n",(0,a.jsx)(s.h5,{id:"example-predictions",children:"Example predictions"}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.img,{alt:"img",src:n(5915).Z+"",width:"1488",height:"985"}),"\n",(0,a.jsx)(s.img,{alt:"img",src:n(7441).Z+"",width:"1488",height:"985"})]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{})})]})}function m(e={}){const{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},2557:(e,s,n)=>{n.d(s,{Z:()=>a});const a=n.p+"assets/images/DANN_Training-be8c290307ff51407eb24d815bca938c.png"},5915:(e,s,n)=>{n.d(s,{Z:()=>a});const a=n.p+"assets/images/Source_domain-11115c151f31c15993ff2af8962e61cc.png"},7441:(e,s,n)=>{n.d(s,{Z:()=>a});const a=n.p+"assets/images/Target_domain-0f76e4ad3541ef01dd5c3aba5f98a7a6.png"},9885:(e,s,n)=>{n.d(s,{Z:()=>a});const a=n.p+"assets/images/gradient_flow_robust-1d124e9fec201b148f276ce2abc8a24f.png"},1151:(e,s,n)=>{n.d(s,{Z:()=>l,a:()=>r});var a=n(7294);const t={},i=a.createContext(t);function r(e){const s=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(i.Provider,{value:s},e.children)}}}]);