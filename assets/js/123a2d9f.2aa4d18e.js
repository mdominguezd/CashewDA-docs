"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[888],{784:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var s=t(5893),a=t(1151);const i={},r=void 0,l={id:"Dataset/ReadyToTrain_DS",title:"ReadyToTrain_DS",description:"Brief description of the submodule",source:"@site/docs/Dataset/ReadyToTrain_DS.md",sourceDirName:"Dataset",slug:"/Dataset/ReadyToTrain_DS",permalink:"/CashewDA-docs/docs/Dataset/ReadyToTrain_DS",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Dataset/ReadyToTrain_DS.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DatasetSplit",permalink:"/CashewDA-docs/docs/Dataset/DatasetSplit"},next:{title:"Transforms",permalink:"/CashewDA-docs/docs/Dataset/Transforms"}},o={},d=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"calculate_percentiles()",id:"calculate_percentiles",level:2},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"get_DataLoaders()",id:"get_dataloaders",level:2},{value:"Size of the dataset:",id:"size-of-the-dataset",level:4},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"Source code",id:"source-code-1",level:3},{value:"get_LOVE_DataLoaders()",id:"get_love_dataloaders",level:2},{value:"Size of the dataset:",id:"size-of-the-dataset-1",level:4},{value:"Params",id:"params-2",level:3},{value:"Outputs",id:"outputs-2",level:3},{value:"Dependencies used",id:"dependencies-used-2",level:3},{value:"Source code",id:"source-code-2",level:3},{value:"Img_Dataset",id:"img_dataset",level:2},{value:"Attributes",id:"attributes",level:3},{value:"Methods",id:"methods",level:3},{value:"__len__()",id:"__len__",level:4},{value:"plot_imgs()",id:"plot_imgs",level:4},{value:"__getitem__()",id:"__getitem__",level:4},{value:"Source code",id:"source-code-3",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,s.jsx)(n.p,{children:"Here all of the code used to pass from the datasets downloaded to ready to use data for training is described."}),"\n",(0,s.jsx)(n.h2,{id:"calculate_percentiles",children:"calculate_percentiles()"}),"\n",(0,s.jsx)(n.p,{children:"Function to calculate 0.01 and 0.99 percentiles of the bands of planet images. These values will be later used for normalizing the dataset."}),"\n",(0,s.jsx)(n.h3,{id:"params",children:"Params"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"img_folder:"})," ",(0,s.jsx)(n.em,{children:"(list)"})," The name of the folder with the images."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"samples:"})," ",(0,s.jsx)(n.em,{children:"(integer)"})," The number of images to take to calculate these percentiles, for computing reasons not all images are considered."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"outputs",children:"Outputs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vals:"})," ",(0,s.jsx)(n.em,{children:"(numpy.ndarray)"})," The mean 1% and 99% quantiles for the images analysed."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\nimport random\nimport numpy as np\nimport rioxarray\n"})}),"\n",(0,s.jsx)(n.h3,{id:"source-code",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def calculate_percentiles(img_folder, samples = 400):\n    """\n        Function to calculate 0.01 and 0.99 percentiles of the bands of planet images. These values will be later used for normalizing the dataset.\n\n        Inputs:\n            - img_folder: The name of the folder with the images.\n            - samples: The number of images to take to calculate these percentiles, for computing reasons not all images are considered.\n        Output:\n            - vals: The mean 1% and 99% quantiles for the images analysed.\n    """\n    imgs = [fn for fn in os.listdir(img_folder) if \'StudyArea\' in fn]\n\n    random.seed(8)\n    img_sample = random.sample(imgs, samples)\n    quantiles = np.zeros((2,4))\n    \n    for i in img_sample:\n        quantiles += rioxarray.open_rasterio(img_folder + "\\\\" + i).quantile((0.01, 0.99), dim = (\'x\',\'y\')).values\n    \n    vals = quantiles/len(img_sample)\n    \n    return vals\n'})}),"\n",(0,s.jsx)(n.h2,{id:"get_dataloaders",children:"get_DataLoaders()"}),"\n",(0,s.jsxs)(n.p,{children:["Function to get the training, validation and test torch.DataLoader or torch.Dataset for a specific dataset. This function gets the images from the ",(0,s.jsx)(n.a,{href:"./ReadyToTrain_DS#img_dataset",children:"Img_Dataset class"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"size-of-the-dataset",children:"Size of the dataset:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"fold"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Domain"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Train"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Validation"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Test"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"1"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"IvoryCoast"})}),(0,s.jsx)(n.td,{children:"8225"}),(0,s.jsx)(n.td,{children:"411"}),(0,s.jsx)(n.td,{children:"38"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"1"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Tanzania"})}),(0,s.jsx)(n.td,{children:"1021"}),(0,s.jsx)(n.td,{children:"57"}),(0,s.jsx)(n.td,{children:"31"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"2"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"IvoryCoast"})}),(0,s.jsx)(n.td,{children:"7770"}),(0,s.jsx)(n.td,{children:"214"}),(0,s.jsx)(n.td,{children:"337"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"2"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Tanzania"})}),(0,s.jsx)(n.td,{children:"1142"}),(0,s.jsx)(n.td,{children:"49"}),(0,s.jsx)(n.td,{children:"29"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"3"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"IvoryCoast"})}),(0,s.jsx)(n.td,{children:"9267"}),(0,s.jsx)(n.td,{children:"466"}),(0,s.jsx)(n.td,{children:"120"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"3"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Tanzania"})}),(0,s.jsx)(n.td,{children:"1142"}),(0,s.jsx)(n.td,{children:"40"}),(0,s.jsx)(n.td,{children:"28"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"params-1",children:"Params"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"dir:"})," (str) Directory with the name of the data to be used."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"batch_size:"})," (int) Size of the batches used for training."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"transform:"})," (torchvision.transforms.V2.Compose) torch composition of transforms used for image augmentation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"normaliztion:"})," (str) Type of normalization used. (Should be 'Linear_1_99')"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VI:"})," (boolean) Boolean indicating if NDVI and NDWI are also used in training."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"split_size:"})," (float) Float between 0 and 1 indicating the fraction of dataset to be used (Especifically useful for HP tuning)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"only_get_DS:"})," (boolean) Boolean for only getting datasets instead of dataloaders."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"train_split_size:"})," (float) fraction of train split to be loaded. (number between 0 and 1)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"val_split_size:"})," (float) fraction of validation and test split to be loaded. (number between 0 and 1)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,s.jsx)(n.p,{children:"Can be either the data loaders:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"train_loader:"})," Training torch data loader"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"val_loader:"})," Validation torch data loader"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"test_loader:"})," Test torch data loader"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"or the datasets:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"train_DS:"})," Training torch data set."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"val_DS:"})," Validation torch data loader"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"test_DS:"})," Test torch data loader"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch.utils.data import random_split\n"})}),"\n",(0,s.jsx)(n.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def get_DataLoaders(dir, batch_size, transform, normalization, VI, only_get_DS = False, train_split_size = None, val_split_size = None):\n    """\n        Function to get the training, validation and test data loader for a specific dataset.\n\n        Inputs:\n            - dir: Directory with the name of the data to be used.\n            - batch_size: Size of the batches used for training.\n            - transform: torch composition of transforms used for image augmentation.\n            - normaliztion: Type of normalization used.\n            - VI: Boolean indicating if NDVI and NDWI are also used in training.\n            - split_size: Float between 0 and 1 indicating the fraction of dataset to be used (Especifically useful for HP tuning)\n            - only_get_DS: Boolean for only getting datasets instead of dataloaders.\n        Output:\n            - train_loader: Training torch data loader\n            - val_loader: Validation torch data loader\n            - test_loader: Test torch data loader\n    """\n    \n    train_DS = Img_Dataset(dir, transform, norm = normalization, VI=VI)\n    val_DS = Img_Dataset(dir, split = \'Validation\', norm = normalization, VI=VI)\n    test_DS = Img_Dataset(dir, split = \'Test\', norm = normalization, VI=VI)\n\n    if train_split_size != None:\n        if val_split_size == None:\n            val_split_size = train_split_size\n            \n        train_DS, l = random_split(train_DS, [train_split_size, 1-train_split_size], generator=torch.Generator().manual_seed(8))\n        val_DS, l = random_split(val_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        test_DS, l = random_split(test_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        \n    train_loader = torch.utils.data.DataLoader(dataset=train_DS, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_DS, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(dataset=test_DS, batch_size=batch_size, shuffle=False)\n    \n    if only_get_DS:\n        return train_DS, val_DS, test_DS\n    else:\n        return train_loader, val_loader, test_loader\n'})}),"\n",(0,s.jsx)(n.h2,{id:"get_love_dataloaders",children:"get_LOVE_DataLoaders()"}),"\n",(0,s.jsxs)(n.p,{children:["Function to get the loaders for LoveDA dataset, which was retrieved using ",(0,s.jsx)(n.a,{href:"https://torchgeo.readthedocs.io/en/stable/api/datasets.html#loveda",children:"torchgeo"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"size-of-the-dataset-1",children:"Size of the dataset:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Domain"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Train"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Validation"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Test"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Urban"})}),(0,s.jsx)(n.td,{children:"1,155"}),(0,s.jsx)(n.td,{children:"677"}),(0,s.jsx)(n.td,{children:"820"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Rural"})}),(0,s.jsx)(n.td,{children:"1,366"}),(0,s.jsx)(n.td,{children:"992"}),(0,s.jsx)(n.td,{children:"976"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"params-2",children:"Params"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"domain:"})," List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"batch_size:"})," Number of images per batch."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"transforms:"})," Image augmentations that will be considered."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"only_get_DS:"})," Boolean for only getting datasets instead of dataloaders."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"train_split_size:"})," Amount of images from training split that will be considered. (Float between 0 and 1)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"val_split_size:"})," Amount of images from validation and test split that will be considered. (Float between 0 and 1)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"outputs-2",children:"Outputs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"train_loader:"})," Training torch LoveDA data loader"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"val_loader:"})," Validation torch LoveDA data loader"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"test_loader:"})," Test torch LoveDA data loader"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"dependencies-used-2",children:"Dependencies used"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torchgeo.datasets import LoveDA\nfrom torch.utils.data import random_split\n"})}),"\n",(0,s.jsx)(n.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def get_LOVE_DataLoaders(domain = ['urban', 'rural'], batch_size = 4, transforms = None, only_get_DS = False, train_split_size = None, val_split_size = None):\n    \"\"\"\n        Function to get the loaders for LoveDA dataset.\n\n        Inputs:\n            - domain: List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'.\n            - batch_size: Number of images per batch.\n            - transforms: Image augmentations that will be considered.\n            - train_split_size: Amount of images from training split that will be considered. (Float between 0 and 1)\n            - val_split_size: Amount of images from validation split that will be considered. (Float between 0 and 1)\n            - only_get_DS: Boolean for only getting datasets instead of dataloaders.\n\n        Output:\n            - train_loader: Training torch LoveDA data loader\n            - val_loader: Validation torch LoveDA data loader\n            - test_loader: Test torch LoveDA data loader\n    \"\"\"\n    if transforms != None:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True, transforms = transforms)\n    else:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True)\n        \n    test_DS = LoveDA('LoveDA', split = 'test', scene = domain, download = True, transforms = transforms)\n    val_DS = LoveDA('LoveDA', split = 'val', scene = domain, download = True, transforms = transforms)\n\n    if train_split_size != None:\n        if val_split_size == None:\n            val_split_size = train_split_size\n        train_DS, l = random_split(train_DS, [train_split_size, 1-train_split_size], generator=torch.Generator().manual_seed(8))\n        val_DS, l = random_split(val_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        test_DS, l = random_split(test_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_DS, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_DS, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(dataset=test_DS, batch_size=batch_size, shuffle=False)\n\n    if only_get_DS:\n        return train_DS, val_DS, test_DS\n    else:\n        return train_loader, val_loader, test_loader\n"})}),"\n",(0,s.jsx)(n.h2,{id:"img_dataset",children:"Img_Dataset"}),"\n",(0,s.jsx)(n.p,{children:"Class to manage the cashew dataset."}),"\n",(0,s.jsx)(n.h3,{id:"attributes",children:"Attributes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"self.img_folder"})," (str) Name of the folder in which the images are stored."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"self.transform"})," (torchvision.transforms.V2.Compose) torch composition of transforms used for image augmentation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"self.split"})," (str) Split of the dataset to be retrieved. Can be Train, Validation or Test."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"self.norm"})," (str) Type of normalization used. Only 'Linear_1_99' is allowed right now."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"self.VI"})," (boolean) Boolean indicating if NDVI and NDWI are also used in training."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,s.jsx)(n.h4,{id:"__len__",children:"__len__()"}),"\n",(0,s.jsx)(n.p,{children:"Method to calculate the number of images in the dataset."}),"\n",(0,s.jsx)(n.h4,{id:"plot_imgs",children:"plot_imgs()"}),"\n",(0,s.jsx)(n.p,{children:"Method to plot a specific image of the dataset."}),"\n",(0,s.jsx)(n.h4,{id:"__getitem__",children:"__getitem__()"}),"\n",(0,s.jsx)(n.p,{children:"Method to get the tensors (image and ground truth) for a specific index (idx)."}),"\n",(0,s.jsx)(n.h3,{id:"source-code-3",children:"Source code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class Img_Dataset(Dataset):\n    \"\"\"\n        Class to manage the cashew dataset.\n    \"\"\"\n    def __init__(self, img_folder, transform = None, split = 'Train', norm = 'Linear_1_99', VI = True, recalculate_perc = False):\n        self.img_folder = img_folder\n        self.transform = transform\n        self.split = split\n        self.norm = norm\n        self.VI = VI\n\n        # Depending of the domain the images will have different attributes (country and quantiles)\n        if 'Tanzania'  in self.img_folder:\n            self.country = 'Tanzania'\n            \n            if recalculate_perc:\n                self.quant_TNZ = calculate_percentiles(img_folder)\n            else:\n                self.quant_TNZ = quant_TNZ\n        else:\n            self.country = 'IvoryCoast'\n            \n            if recalculate_perc:\n                self.quant_CIV = calculate_percentiles(img_folder)\n            else:\n                self.quant_CIV = quant_CIV\n\n    def __len__(self):\n        \"\"\"\n            Method to calculate the number of images in the dataset.    \n        \"\"\"\n        return sum([self.split in i for i in os.listdir(self.img_folder)])//2\n\n    def plot_imgs(self, idx, VIs = False):\n        \"\"\"\n            Method to plot a specific image of the dataset.\n            \n            Input:\n                - self: The dataset class and its attributes.\n                - idx: index of the image that will be plotted.\n                - VIs: Boolean describing if vegetation indices should be plotted\n        \"\"\"\n\n        im, g = self.__getitem__(idx)\n\n        if VIs:\n            fig, ax = plt.subplots(2,2,figsize = (12,12))\n\n            ax[0,0].imshow(im[[2,1,0],:,:].permute(1,2,0))\n            ax[0,0].set_title('Planet image')\n            ax[0,1].imshow(g[0,:,:])\n            ax[0,1].set_title('Cashew crops GT')\n\n            VIs = im[4:6]\n\n            g1=ax[1,0].imshow(VIs[0], cmap = plt.cm.get_cmap('RdYlGn', 5), vmin = 0, vmax = 1)\n            ax[1,0].set_title('NDVI')\n            fig.colorbar(g1)\n            g2=ax[1,1].imshow(VIs[1], cmap = plt.cm.get_cmap('Blues_r', 5), vmin = 0, vmax = 1)\n            ax[1,1].set_title('NDWI')\n            fig.colorbar(g2)\n\n        else:\n            fig, ax = plt.subplots(1,2,figsize = (12,6))\n\n            ax[0].imshow(im[[2,1,0],:,:].permute(1,2,0))\n            ax[0].set_title('Planet image')\n            ax[1].imshow(g[0,:,:])\n            ax[1].set_title('Cashew crops GT')\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n            Method to get the tensors (image and ground truth) for a specific image.\n        \"\"\"\n    \n        conversion = T.ToTensor()\n\n        img = io.imread(fname = self.img_folder + '/Cropped' + self.country + self.split + 'StudyArea_{:05d}'.format(idx) + '.tif').astype(np.float32)\n\n        if self.VI:\n            if self.norm == 'Linear_1_99':\n                ndvi = (img[:,:,3] - img[:,:,2])/(img[:,:,3] + img[:,:,2]) \n                ndwi = (img[:,:,1] - img[:,:,3])/(img[:,:,3] + img[:,:,1])\n\n        if self.norm == 'Linear_1_99':\n            for i in range(img.shape[-1]):\n                if 'Tanz' in self.img_folder:\n                    img[:,:,i] = (img[:,:,i] - self.quant_TNZ[0,i])/(self.quant_TNZ[1,i] - self.quant_TNZ[0,i])\n                elif 'Ivor' in self.img_folder:\n                    img[:,:,i] = (img[:,:,i] - self.quant_CIV[0,i])/(self.quant_CIV[1,i] - self.quant_CIV[0,i])\n\n        if self.VI:\n            ndvi = np.expand_dims(ndvi, axis = 2)\n            ndwi = np.expand_dims(ndwi, axis = 2)\n            img = np.concatenate((img, ndvi, ndwi), axis = 2)\n\n        img = conversion(img).float()\n\n        img = torchvision.tv_tensors.Image(img)\n\n        GT = io.imread(fname = self.img_folder + '/Cropped' + self.country + self.split + 'GT_{:05d}'.format(idx) + '.tif').astype(np.float32)\n\n        GT = torch.flip(conversion(GT), dims = (1,))\n\n        GT = torchvision.tv_tensors.Image(GT)\n\n        if self.transform != None:\n            GT, img = self.transform(GT, img)\n\n        return img, GT\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>r});var s=t(7294);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);