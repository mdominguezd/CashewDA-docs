"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[479],{7798:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>o,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var n=a(5893),t=a(1151);const r={sidebar_position:3},i=void 0,l={id:"Training/Train_DANN",title:"Train_DANN",description:"Brief description of the submodule",source:"@site/docs/Training/Train_DANN.md",sourceDirName:"Training",slug:"/Training/Train_DANN",permalink:"/CashewDA-docs/docs/Training/Train_DANN",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Training/Train_DANN.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Train_DomainOnly",permalink:"/CashewDA-docs/docs/Training/Train_DomainOnly"},next:{title:"Validate",permalink:"/CashewDA-docs/docs/category/validate"}},o={},c=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"initialize_Unet_DANN()",id:"initialize_unet_dann",level:2},{value:"Number of parameters for LoveDA using:",id:"number-of-parameters-for-loveda-using",level:4},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"evaluate()",id:"evaluate",level:2},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"Source code",id:"source-code-1",level:3},{value:"DANN_training_loop()",id:"dann_training_loop",level:2},{value:"Params",id:"params-2",level:3},{value:"Outputs",id:"outputs-2",level:3},{value:"Source code",id:"source-code-2",level:3},{value:"train_full_DANN()",id:"train_full_dann",level:2},{value:"Params",id:"params-3",level:3},{value:"Outputs",id:"outputs-3",level:3},{value:"Source code",id:"source-code-3",level:3}];function d(e){const s={a:"a",annotation:"annotation",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,n.jsx)(s.p,{children:"In this submodule, all of the functions used for training the models with domain adaptation (DANN) are described in detail."}),"\n",(0,n.jsx)(s.h2,{id:"initialize_unet_dann",children:"initialize_Unet_DANN()"}),"\n",(0,n.jsx)(s.p,{children:"Function to initialize U-Net and the discriminator that will be trained using UNet-DANN"}),"\n",(0,n.jsx)(s.h4,{id:"number-of-parameters-for-loveda-using",children:"Number of parameters for LoveDA using:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.code,{children:"starter_channels"})," = 16"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.code,{children:"attention"})," = True"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.code,{children:"up_layer"})," = 4"]}),"\n"]}),"\n",(0,n.jsxs)(s.table,{children:[(0,n.jsx)(s.thead,{children:(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.th,{children:(0,n.jsx)(s.strong,{children:"Part of the model"})}),(0,n.jsx)(s.th,{children:(0,n.jsx)(s.strong,{children:"# of parameters"})})]})}),(0,n.jsxs)(s.tbody,{children:[(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"Feature extractor"}),(0,n.jsx)(s.td,{children:"1'103,524"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"Classifier"}),(0,n.jsx)(s.td,{children:"136"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"Discriminator"}),(0,n.jsx)(s.td,{children:"134'546,496"})]})]})]}),"\n",(0,n.jsx)(s.h3,{id:"params",children:"Params"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"n_channels: Number of channels of input images."}),"\n",(0,n.jsx)(s.li,{children:"n_classes: Number of classes to be segmented on the images."}),"\n",(0,n.jsx)(s.li,{children:"bilinear: Boolean used for upsamplimg method. (True: Bilinear is used. False: Transpose convolution is used.) [Default = True]"}),"\n",(0,n.jsx)(s.li,{children:"starter: Start number of channels of the UNet. [Default = 16]"}),"\n",(0,n.jsx)(s.li,{children:"up_layer: Upward step layer in which the U_Net is divided into Feature extractor and Classifier. [Default = 4]"}),"\n",(0,n.jsx)(s.li,{children:"attention: Boolean that describes if attention gates in the UNet will be used or not. [Default = True]"}),"\n",(0,n.jsx)(s.li,{children:"Love: Boolean to indicate if LoveDA dataset is being used."}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"outputs",children:"Outputs"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"network: U-Net+DANN architecture to be trained."}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-py",children:"import torch\n\nfrom Models.U_Net import UNetDANN\nfrom utils import get_training_device\n"})}),"\n",(0,n.jsx)(s.h3,{id:"source-code",children:"Source code"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:'def initialize_Unet_DANN(n_channels, n_classes, bilinear = True, starter = 16, up_layer = 4, attention = True, Love = False, grad_rev_w = 1):\n    """\n        Function to initialize U-Net and the discriminator that will be trained using UNet-DANN\n\n        Inputs:\n            - n_channels: Number of channels of input images.\n            - n_classes: Number of classes to be segmented on the images.\n            - bilinear: Boolean used for upsamplimg method. (True: Bilinear is used. False: Transpose convolution is used.) [Default = True]\n            - starter: Start number of channels of the UNet. [Default = 16]\n            - up_layer: Upward step layer in which the U_Net is divided into Feature extractor and Classifier. [Default = 4]\n            - attention: Boolean that describes if attention gates in the UNet will be used or not. [Default = True]\n\n        Outputs:\n            - network: U-Net+DANN architecture to be trained.\n    """\n    device = get_training_device()\n\n    # Calculate the number  of features that go in the fully connected layers of the discriminator\n    if Love:\n        img_size = 256\n    else:\n        img_size = 256\n        \n    in_feat = (img_size//(2**4))**2 * starter*(2**3) \n    \n    seed = 123\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    network = UNetDANN(n_channels=n_channels, n_classes=n_classes, bilinear = bilinear, starter = starter, up_layer = up_layer, attention = attention, DA = True, in_feat = in_feat, grad_rev_w = grad_rev_w).to(device)\n\n    return network\n'})}),"\n",(0,n.jsx)(s.h2,{id:"evaluate",children:"evaluate()"}),"\n",(0,n.jsx)(s.h3,{id:"params-1",children:"Params"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"net: Pytorch network that will be evaluated."}),"\n",(0,n.jsx)(s.li,{children:"validate_loader: Validation (or Test) dataset with which the network will be evaluated."}),"\n",(0,n.jsx)(s.li,{children:"loss_function: Loss function used to evaluate the network."}),"\n",(0,n.jsx)(s.li,{children:"accu_function: Accuracy function used to evaluate the network."}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"metric: List with loss and accuracy values calculated for the validation/test dataset."}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-py",children:"import numpy as np\nimport torch\n\nfrom utils import get_training_device, LOVE_resample_fly\n"})}),"\n",(0,n.jsx)(s.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"def evaluate(net, validate_loader, loss_function, accu_function = BinaryF1Score(), Love = False, binary_love = False, revgrad = 1):\n    \"\"\"\n        Function to evaluate the performance of a network on a validation data loader.\n\n        Inputs:\n            - net: Pytorch network that will be evaluated.\n            - validate_loader: Validation (or Test) dataset with which the network will be evaluated.\n            - loss_function: Loss function used to evaluate the network.\n            - accu_function: Accuracy function used to evaluate the network.\n\n        Output:\n            - metric: List with loss and accuracy values calculated for the validation/test dataset.\n    \"\"\"\n    \n    net.eval()  # Set the model to evaluation mode\n    device = get_training_device()\n\n    f1_scores = []\n    losses = []\n\n    with torch.no_grad():\n        # Iterate over validate loader to get mean accuracy and mean loss\n        for i, Data in enumerate(validate_loader):\n            \n            # The inputs and GT are obtained differently depending of the Dataset (LoveDA or our own DS)\n            if Love:\n                inputs = LOVE_resample_fly(Data['image'])\n                GTs = LOVE_resample_fly(Data['mask'])\n                if binary_love:\n                    GTs = (GTs == 6).long()\n                    \n            else:\n                inputs = Data[0]\n                GTs = Data[1]\n        \n            inputs = inputs.to(device)\n            GTs = GTs.type(torch.long).squeeze().to(device)\n            pred = net(inputs, revgrad)[0]\n        \n            f1 = accu_function.to(device)\n        \n            if (pred.max(1)[1].shape != GTs.shape):\n                GTs = GTs[None, :, :]\n\n            loss = loss_function(pred, GTs)/GTs.shape[0]\n            \n            f1_score = f1(pred.max(1)[1], GTs)\n            \n            f1_scores.append(f1_score.to('cpu').numpy())\n            losses.append(loss.to('cpu').numpy())\n\n        metric = [np.nanmean(f1_scores), np.nanmean(losses)]   \n        \n    return metric\n"})}),"\n",(0,n.jsx)(s.h2,{id:"dann_training_loop",children:"DANN_training_loop()"}),"\n",(0,n.jsx)(s.p,{children:"Function to carry out the training loop for UNet-DANN."}),"\n",(0,n.jsxs)(s.p,{children:["The implementation of DANN was based on the code found ",(0,n.jsx)(s.a,{href:"https://github.com/jvanvugt/pytorch-domain-adaptation/blob/master/revgrad.py",children:"here"}),"."]}),"\n",(0,n.jsx)(s.h3,{id:"params-2",children:"Params"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"source_domain"})," Either str with name of the source domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"target_domain"})," Either str with name of the target domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"DS_args"})," List of arguments for dataset creation."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"network_args"})," List of arguments for neural network creation. (e.g. n_classes, bilinear, starter, up_layer, attention)"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"optim_args:"})," List of arguments for the optimizer (e.g. Learning rates and momentum)"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"DA_args"})," List of arguments for the application of domain adaptation (e.g. epochs, e_0, lambda_max)"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"Love"})," default = False"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"binary_love"})," default = False"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"seg_loss"})," default = torch.nn.CrossEntropyLoss()"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"domain_loss"})," default = torch.nn.BCEWithLogitsLoss()"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"accu_function"})," default = BinaryF1Score()"]}),"\n"]}),"\n",(0,n.jsx)(s.span,{className:"katex-display",children:(0,n.jsxs)(s.span,{className:"katex",children:[(0,n.jsx)(s.span,{className:"katex-mathml",children:(0,n.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,n.jsxs)(s.semantics,{children:[(0,n.jsxs)(s.mrow,{children:[(0,n.jsx)(s.mi,{children:"\u03bb"}),(0,n.jsx)(s.mo,{children:"="}),(0,n.jsx)(s.mi,{children:"m"}),(0,n.jsx)(s.mi,{children:"a"}),(0,n.jsx)(s.mi,{children:"x"}),(0,n.jsxs)(s.mrow,{children:[(0,n.jsx)(s.mo,{fence:"true",children:"("}),(0,n.jsx)(s.mn,{children:"0"}),(0,n.jsx)(s.mo,{separator:"true",children:","}),(0,n.jsxs)(s.msub,{children:[(0,n.jsx)(s.mi,{children:"\u03bb"}),(0,n.jsxs)(s.mrow,{children:[(0,n.jsx)(s.mi,{children:"m"}),(0,n.jsx)(s.mi,{children:"a"}),(0,n.jsx)(s.mi,{children:"x"})]})]}),(0,n.jsx)(s.mo,{children:"\u22c5"}),(0,n.jsxs)(s.mfrac,{children:[(0,n.jsxs)(s.mrow,{children:[(0,n.jsx)(s.mi,{children:"e"}),(0,n.jsx)(s.mi,{children:"p"}),(0,n.jsx)(s.mi,{children:"o"}),(0,n.jsx)(s.mi,{children:"c"}),(0,n.jsx)(s.mi,{children:"h"}),(0,n.jsx)(s.mo,{children:"\u2212"}),(0,n.jsxs)(s.msub,{children:[(0,n.jsx)(s.mi,{children:"e"}),(0,n.jsx)(s.mn,{children:"0"})]})]}),(0,n.jsxs)(s.mrow,{children:[(0,n.jsx)(s.mi,{children:"e"}),(0,n.jsx)(s.mi,{children:"p"}),(0,n.jsx)(s.mi,{children:"o"}),(0,n.jsx)(s.mi,{children:"c"}),(0,n.jsx)(s.mi,{children:"h"}),(0,n.jsx)(s.mi,{children:"s"}),(0,n.jsx)(s.mo,{children:"\u2212"}),(0,n.jsxs)(s.msub,{children:[(0,n.jsx)(s.mi,{children:"e"}),(0,n.jsx)(s.mn,{children:"0"})]})]})]}),(0,n.jsx)(s.mo,{fence:"true",children:")"})]})]}),(0,n.jsx)(s.annotation,{encoding:"application/x-tex",children:"    \\lambda = max\\left(0, \\lambda_{max} \\cdot \\frac{epoch - e_0}{epochs - e_0}\\right)"})]})})}),(0,n.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,n.jsxs)(s.span,{className:"base",children:[(0,n.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"\u03bb"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,n.jsx)(s.span,{className:"mrel",children:"="}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,n.jsxs)(s.span,{className:"base",children:[(0,n.jsx)(s.span,{className:"strut",style:{height:"2.4em",verticalAlign:"-0.95em"}}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"ma"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,n.jsxs)(s.span,{className:"minner",children:[(0,n.jsx)(s.span,{className:"mopen delimcenter",style:{top:"0em"},children:(0,n.jsx)(s.span,{className:"delimsizing size3",children:"("})}),(0,n.jsx)(s.span,{className:"mord",children:"0"}),(0,n.jsx)(s.span,{className:"mpunct",children:","}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mord mathnormal",children:"\u03bb"}),(0,n.jsx)(s.span,{className:"msupsub",children:(0,n.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,n.jsxs)(s.span,{className:"vlist-r",children:[(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.1514em"},children:(0,n.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,n.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,n.jsxs)(s.span,{className:"mord mtight",children:[(0,n.jsx)(s.span,{className:"mord mathnormal mtight",children:"ma"}),(0,n.jsx)(s.span,{className:"mord mathnormal mtight",children:"x"})]})})]})}),(0,n.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,n.jsx)(s.span,{className:"vlist-r",children:(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,n.jsx)(s.span,{})})})]})})]}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsx)(s.span,{className:"mbin",children:"\u22c5"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,n.jsx)(s.span,{className:"mfrac",children:(0,n.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,n.jsxs)(s.span,{className:"vlist-r",children:[(0,n.jsxs)(s.span,{className:"vlist",style:{height:"1.3714em"},children:[(0,n.jsxs)(s.span,{style:{top:"-2.314em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"oc"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"h"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,n.jsx)(s.span,{className:"msupsub",children:(0,n.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,n.jsxs)(s.span,{className:"vlist-r",children:[(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,n.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,n.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,n.jsx)(s.span,{className:"mord mtight",children:"0"})})]})}),(0,n.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,n.jsx)(s.span,{className:"vlist-r",children:(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,n.jsx)(s.span,{})})})]})})]})]})]}),(0,n.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,n.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,n.jsxs)(s.span,{style:{top:"-3.677em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"oc"}),(0,n.jsx)(s.span,{className:"mord mathnormal",children:"h"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,n.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,n.jsxs)(s.span,{className:"mord",children:[(0,n.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,n.jsx)(s.span,{className:"msupsub",children:(0,n.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,n.jsxs)(s.span,{className:"vlist-r",children:[(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,n.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,n.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,n.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,n.jsx)(s.span,{className:"mord mtight",children:"0"})})]})}),(0,n.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,n.jsx)(s.span,{className:"vlist-r",children:(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,n.jsx)(s.span,{})})})]})})]})]})]})]}),(0,n.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,n.jsx)(s.span,{className:"vlist-r",children:(0,n.jsx)(s.span,{className:"vlist",style:{height:"0.8804em"},children:(0,n.jsx)(s.span,{})})})]})}),(0,n.jsx)(s.span,{className:"mclose nulldelimiter"})]}),(0,n.jsx)(s.span,{className:"mclose delimcenter",style:{top:"0em"},children:(0,n.jsx)(s.span,{className:"delimsizing size3",children:")"})})]})]})]})]})}),"\n",(0,n.jsx)(s.h3,{id:"outputs-2",children:"Outputs"}),"\n",(0,n.jsx)(s.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"def DANN_training_loop(source_domain, target_domain, DS_args, network_args, optim_args, DA_args, Love = False, binary_love = False, seg_loss = torch.nn.CrossEntropyLoss(), domain_loss = torch.nn.BCEWithLogitsLoss(), accu_function = BinaryF1Score(), semi = False, semi_perc = 0.1):\n    \"\"\"\n        Function to carry out the training of UNet-DANN.\n\n        Inputs:\n            - source_domain: Either str with name of the source domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset.\n            - target_domain: Either str with name of the target domain ('IvoryCoast' or 'Tanzania') for own dataset or list with a str that has the name of the domain (['rural'] or ['urban']) for LoveDA dataset.\n            - DS_args: List of arguments for dataset creation. \n                - For LoveDA: Should at least have: (batch_size, transforms, only_get_DS)\n                - For own dataset: Should at least have: (batch_size, transform, normalization, VI, only_get_DS)\n            - network_args: List of arguments for neural network creation. (e.g. n_classes, bilinear, starter, up_layer, attention)\n    \"\"\"\n\n    device = get_training_device()\n\n    if Love:\n        if len(DS_args) < 3:\n            raise Exception(\"The length of DS_args should be equal or greater than 3. Check the documentation for LoveDA.\")\n        source_DSs = get_LOVE_DataLoaders(source_domain, *DS_args)\n        target_DSs = get_LOVE_DataLoaders(target_domain, *DS_args)\n        n_channels = source_DSs[0].__getitem__(0)['image'].size()[-3]\n        \n    else:\n        if len(DS_args) < 5:\n            raise Exception(\"The length of DS_args should be equal or greater than 5. Check the documentation for own Dataset.\")\n        source_DSs = get_DataLoaders(source_domain, *DS_args)\n        target_DSs = get_DataLoaders(target_domain, *DS_args)\n        n_channels = source_DSs[0].__getitem__(0)[0].size()[-3]\n\n    source_train_dataset = source_DSs[0]\n    target_train_dataset = target_DSs[0]\n    batch_size = DS_args[0]\n\n    # Calculate number of batch iterations using both domains (The number of times the smallest dataset will need to be re-used for training.)\n    source_n_batches = np.ceil(len(source_train_dataset)/(batch_size//2))\n    target_n_batches = np.ceil(len(target_train_dataset)/(batch_size//2))\n    \n    n_batches = min(source_n_batches, target_n_batches)\n    \n    batch_iterations = np.ceil(max(source_n_batches, target_n_batches) / n_batches)\n\n    # Create validation data loaders\n    source_val_loader = torch.utils.data.DataLoader(dataset=source_DSs[1], batch_size=batch_size, shuffle=False)\n    target_val_loader = torch.utils.data.DataLoader(dataset=target_DSs[1], batch_size=batch_size, shuffle=False)\n    \n    # Initialize the networks to be trained and the optimizer\n    network = initialize_Unet_DANN(n_channels, *network_args)\n    \n    weight_decay = 1e-4\n    \n    if len(optim_args) == 3:\n            optim = torch.optim.SGD([{'params': network.FE.parameters(), 'lr': optim_args[0]},\n                                     {'params': network.C.parameters(), 'lr': optim_args[0]},\n                                     {'params': network.D.parameters(), 'lr': optim_args[1]},\n                                     ], weight_decay = weight_decay, momentum = optim_args[2])\n    elif len(optim_args) > 1:\n        optim = torch.optim.Adam([{'params': network.FE.parameters(), 'lr': optim_args[0]},\n                                  {'params': network.C.parameters(), 'lr': optim_args[0]},\n                                  {'params': network.D.parameters(), 'lr': optim_args[1]},\n                                 ], weight_decay = weight_decay)\n        \n    else:\n        optim = torch.optim.Adam([{'params': network.FE.parameters(), 'lr': optim_args[0]},\n                                  {'params': network.C.parameters(), 'lr': optim_args[0]},\n                                  {'params': network.D.parameters(), 'lr': optim_args[0]},\n                                 ], weight_decay = weight_decay)\n        \n    # Create empty lists where segementation accuracy in source dataset and segmentation and domain loss will be stored.\n    val_accuracy = []\n    val_disc_accu = []\n    val_accuracy_target = []\n    segmen_loss_l = []\n    train_accuracy_l = []\n    train_disc_accuracy_l = []\n    domain_loss_l = []\n\n    eps = []\n\n    source_loader = torch.utils.data.DataLoader(dataset=source_train_dataset, batch_size=batch_size//2, shuffle=True)\n    target_loader = torch.utils.data.DataLoader(dataset=target_train_dataset, batch_size=batch_size//2, shuffle=True)\n\n    if semi:\n        sub = torch.utils.data.Subset(target_train_dataset, list(range(int(len(target_train_dataset)//(1/semi_perc)))))\n        target_loader_sub = torch.utils.data.DataLoader(dataset=sub, batch_size=batch_size//2, shuffle=True)\n\n    epochs, e_0, l_max = DA_args\n\n    for epoch in tqdm(range(epochs), desc = 'Training UNet-DANN model'):\n\n        batches = zip(source_loader, target_loader)\n        n_batches = min(len(source_loader), len(target_loader))\n\n        total_domain_loss = total_seg_accuracy = total_seg_loss = 0\n        \n        revgrad = np.max([0, l_max*(epoch - e_0)/(epochs - e_0)])\n\n        for source, target in tqdm(batches, disable=True, total=n_batches):\n            \n            if Love:\n                source_img = LOVE_resample_fly(source['image'])\n                source_msk = LOVE_resample_fly(source['mask'])\n                \n                target_img = LOVE_resample_fly(target['image'])\n                target_msk = LOVE_resample_fly(target['mask'])\n\n            else:\n                source_img = source[0]\n                source_msk = source[1][:,0,:,:].to(torch.int64)\n                \n                target_img = target[0]\n                target_msk = target[1]\n                \n                \n\n            imgs = torch.cat([source_img, target_img])\n            imgs = imgs.to(device)\n\n            domain_gt = torch.cat([torch.ones(source_img.shape[0]),\n                                   torch.zeros(target_img.shape[0])])\n            \n            domain_gt = domain_gt.to(device)\n            mask_gt = source_msk.to(device)\n\n            features = network.FE(imgs)\n            dw = network.FE.DownSteps(imgs)\n            \n            seg_preds = network.C(features, dw)\n            dom_preds = network.D(features, revgrad)\n            \n            # Calculate the loss function\n            segmentation_loss = seg_loss(seg_preds[:source_img.shape[0]], mask_gt)\n            discriminator_loss = domain_loss(dom_preds.squeeze(), domain_gt)\n\n            if semi:\n                \n                target_sub = next(iter(target_loader_sub))\n                    \n                semitarget_img = target_sub[0].to(device)\n                semitarget_gt = target_sub[1][:,0,:,:].to(torch.int64).to(device)\n                \n                features = network.FE(semitarget_img)\n                dw = network.FE.DownSteps(semitarget_img)\n\n                semi_preds = network.C(features, dw)\n\n                seg_batch = seg_loss(semi_preds, semitarget_gt)\n                \n                segmentation_loss += seg_batch\n\n            seg_imp = 1\n            \n            # Total loss\n            loss = seg_imp*segmentation_loss + (2-seg_imp)*discriminator_loss\n\n            # set the gradients of the model to 0 and perform the backward propagation\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            total_domain_loss += discriminator_loss.item()\n            total_seg_loss += segmentation_loss.item()\n            accu_function = accu_function.to(device)\n            accu = accu_function(seg_preds[:source_img.shape[0]].max(1)[1], mask_gt)\n            total_seg_accuracy += accu.item()\n\n        dom_loss = total_domain_loss / n_batches\n        segmentation_loss = total_seg_loss / n_batches\n        seg_accuracy = total_seg_accuracy / n_batches\n\n        print('dom_loss, seg_loss, seg_accu', dom_loss, segmentation_loss, seg_accuracy)\n\n        if (epoch//10 == epoch/10):\n            #After 4 epochs, reduce the learning rate by a factor \n            optim.param_groups[0]['lr'] *= 0.75\n            oa_val = 1\n            # evaluate_disc(network, [source_loader, target_val_loader], device, Love)\n        \n        # Evaluate network on validation dataset\n        f1_val, loss_val = evaluate(network, source_val_loader, seg_loss, accu_function, Love, binary_love, revgrad)\n        val_accuracy.append(f1_val)\n    \n        f1_val_target, loss_val_target = evaluate(network, target_val_loader, seg_loss, accu_function, Love, binary_love, revgrad)\n        val_accuracy_target.append(f1_val_target)\n        \n        #update values in DS_args for cosine similarity computation\n        # DS_args[-3] = False\n        # DS_args[-2] = 0.025\n        # DS_args[-1] = 0.025\n        \n        \n\n        eps.append(epoch + 1)\n        val_disc_accu.append(oa_val)\n        segmen_loss_l.append(segmentation_loss)\n        train_accuracy_l.append(seg_accuracy)\n        domain_loss_l.append(dom_loss)\n\n        # Selection of best model so far using validation dataset.\n        # Relative importance of segmentation over discrimination (0 to 5)\n        rel_imp_seg = 3\n\n        overall = ((5-rel_imp_seg)*(dom_loss) + rel_imp_seg*(f1_val))/5\n\n        print('disc_accu, f1val, overall', oa_val, f1_val, overall)\n\n        if epoch == 0:\n            best_model_f1 = f1_val\n            best_oa = oa_val\n            best_overall = overall\n            target_f1 = f1_val_target\n            torch.save(network, 'BestDANNModel.pt')\n            best_network = network\n        else:\n            if best_overall < overall:\n                best_model_f1 = f1_val\n                best_oa = oa_val\n                best_overall = overall\n                print(best_overall)\n                target_f1 = f1_val_target\n                torch.save(network, 'BestDANNModel.pt')\n                best_network = network\n\n        if epoch == e_0:\n            best_DA_f1 = f1_val\n            torch.save(network, 'BestDANNModelAfter_e0.pt')\n            best_network = network\n        elif epoch > e_0:\n            if best_DA_f1 < f1_val:\n                best_DA_f1 = f1_val\n                torch.save(network, 'BestDANNModelAfter_e0.pt')\n                best_network = network\n                \n        fig = plt.figure(figsize = (7,5))\n        \n        plt.plot(eps, segmen_loss_l, '-k', label = 'Segmentation loss')\n        plt.plot(eps, domain_loss_l, '-r', label = 'Domain loss')\n        plt.plot(eps, train_accuracy_l, '--g', label = 'Train segmentation accuracy')\n        plt.axvline(x = e_0, color = 'darkred', label = 'e_0')\n\n        plt.plot(eps, val_accuracy, label = 'Source domain val accuracy')\n        plt.plot(eps, val_accuracy_target, label = 'Target domain val accuracy')\n        # plt.plot(eps, val_disc_accu, label = 'Discrimination accuracy')\n        # plt.plot(eps, train_disc_accuracy_l, '--y', label = 'Train discriminator accuracy')\n\n        plt.ylim((0,1.1))\n        plt.xlabel('Epoch')\n\n        plt.legend()\n\n        fig.savefig('DANN_Training.png', dpi = 100)\n        plt.close()\n\n    training_list = pd.DataFrame([eps, segmen_loss_l, domain_loss_l, train_accuracy_l, val_accuracy, val_accuracy_target, val_disc_accu])\n\n    training_list.to_csv('Training_loop.csv')\n\n    torch.save(network, 'LastDANNModel.pt')\n\n    return best_model_f1, target_f1, best_overall, best_network, training_list\n"})}),"\n",(0,n.jsx)(s.h2,{id:"train_full_dann",children:"train_full_DANN()"}),"\n",(0,n.jsx)(s.p,{children:"Aggregating function to run all of the functions meant to train a model with domain adaptation (Source domain = Ivory coast and Target domain = Tanzania)."}),"\n",(0,n.jsx)(s.h3,{id:"params-3",children:"Params"}),"\n",(0,n.jsx)(s.h3,{id:"outputs-3",children:"Outputs"}),"\n",(0,n.jsx)(s.h3,{id:"source-code-3",children:"Source code"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-py",children:"def train_full_DANN():\n    \n    DS_args = [8, get_transforms(), 'Linear_1_99', True, True, None, None]\n\n    ## Related to the network\n    n_classes = 2\n    bilinear = True\n    sts = 16\n    up_layer = 4\n    att = True\n    \n    network_args = [n_classes, bilinear, sts, up_layer, att]\n    \n    lr_s = 0.0001\n    lr_d = 0.0001\n    \n    optim_args = [lr_s, lr_d]\n    \n    epochs = 80\n    e_0 = 40\n    l_max = 0.1\n    \n    DA_args = [epochs, e_0, l_max]\n    \n    best_model_f1, target_f1, best_overall, best_network, training_list = DANN_training_loop('IvoryCoastSplit1', 'TanzaniaSplit1', DS_args, network_args, optim_args, DA_args, seg_loss = FocalLoss(4))\n"})})]})}function m(e={}){const{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},1151:(e,s,a)=>{a.d(s,{Z:()=>l,a:()=>i});var n=a(7294);const t={},r=n.createContext(t);function i(e){const s=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),n.createElement(r.Provider,{value:s},e.children)}}}]);