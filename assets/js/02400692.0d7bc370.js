"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[762],{3351:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var t=a(5893),r=a(1151);const i={sidebar_position:2},o=void 0,s={id:"Training/Train_DomainOnly",title:"Train_DomainOnly",description:"Brief description of the submodule",source:"@site/docs/Training/Train_DomainOnly.md",sourceDirName:"Training",slug:"/Training/Train_DomainOnly",permalink:"/CashewDA-docs/docs/Training/Train_DomainOnly",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Training/Train_DomainOnly.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Hyper_Parameter_Tuning",permalink:"/CashewDA-docs/docs/Training/Hyper_Parameter_Tuning"},next:{title:"Train_DANN",permalink:"/CashewDA-docs/docs/Training/Train_DANN"}},l={},d=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"evaluate()",id:"evaluate",level:2},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"training_loop()",id:"training_loop",level:2},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"Source code",id:"source-code-1",level:3},{value:"train_3fold_DomainOnly()",id:"train_3fold_domainonly",level:2},{value:"Params",id:"params-2",level:3},{value:"Outputs",id:"outputs-2",level:3},{value:"Dependencies used",id:"dependencies-used-2",level:3},{value:"Source code",id:"source-code-2",level:3},{value:"train_LoveDA_DomainOnly()",id:"train_loveda_domainonly",level:2},{value:"Params",id:"params-3",level:3},{value:"Outputs",id:"outputs-3",level:3},{value:"Dependencies used",id:"dependencies-used-3",level:3},{value:"Source code",id:"source-code-3",level:3},{value:"run_DomainOnly()",id:"run_domainonly",level:2},{value:"Params",id:"params-4",level:3},{value:"Outputs",id:"outputs-4",level:3},{value:"Source code",id:"source-code-4",level:3}];function c(n){const e={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,t.jsx)(e.p,{children:"In this submodule, all of the functions used for training the domain-only models are described in detail."}),"\n",(0,t.jsx)(e.h2,{id:"evaluate",children:"evaluate()"}),"\n",(0,t.jsx)(e.p,{children:"Function used to evaluate the segmentation performance of a specified network. It gets the predictions calculated using the network for a specified data loader and calculates the mean loss and accuracy comparing the predictions with the ground truth labels of the loader."}),"\n",(0,t.jsx)(e.h3,{id:"params",children:"Params"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"net:"})," (torch.nn.Module) Network class used to get the segmentation predictions."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"validate_loader:"}),"  (torch.nn.DataLoader) Data loader of which the mean loss and accuracy will be calculated."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"loss_function:"})," (torch.nn.Module) Loss function used during the training of the network."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"accu_function:"})," (torchmetrics.classification) Function to calculate the mean accuracy of the network on validate_loader. ",(0,t.jsx)(e.em,{children:"Default"})," is BinaryF1Score()"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Love:"})," Binary to indicate if working with LoveDA dataset."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"binary_love:"})," Binary to indicate if working with only one class of LoveDA dataset."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"outputs",children:"Outputs"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"metric:"})," (list) List with the mean values of accuracy and loss."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\nfrom torchmetrics.classification import BinaryF1Score\n\nfrom utils import LOVE_resample_fly\n"})}),"\n",(0,t.jsx)(e.h3,{id:"source-code",children:"Source code"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'\ndef evaluate(net, validate_loader, loss_function, accu_function = BinaryF1Score(), Love = False, binary_love = False):\n    """\n        Function to evaluate the performance of a network on a validation data loader.\n\n        Inputs:\n            - net: Pytorch network that will be evaluated.\n            - validate_loader: Validation (or Test) dataset with which the network will be evaluated.\n            - loss_function: Loss function used to evaluate the network.\n            - accu_function: Accuracy function used to evaluate the network.\n\n        Output:\n            - metric: List with loss and accuracy values calculated for the validation/test dataset.\n    """\n    \n    net.eval()  # Set the model to evaluation mode\n    device = next(iter(net.parameters())).device # Get training device ("cuda" or "cpu")\n\n    f1_scores = []\n    losses = []\n\n    with torch.no_grad():\n        # Iterate over validate loader to get mean accuracy and mean loss\n        for i, Data in enumerate(validate_loader):\n            \n            # The inputs and GT are obtained differently depending of the Dataset (LoveDA or our own DS)\n            if Love:\n                inputs = LOVE_resample_fly(Data[\'image\'])\n                GTs = LOVE_resample_fly(Data[\'mask\'])\n                if binary_love:\n                    GTs = (GTs == 6).long()\n            else:\n                inputs = Data[0]\n                GTs = Data[1]\n        \n\n            inputs = inputs.to(device)\n            GTs = GTs.type(torch.long).squeeze().to(device)\n            pred = net(inputs)\n        \n            f1 = accu_function.to(device)\n        \n            if (pred.max(1)[1].shape != GTs.shape):\n                GTs = GTs[None, :, :]\n\n            loss = loss_function(pred, GTs)/GTs.shape[0]\n        \n            f1_score = f1(pred.max(1)[1], GTs)\n            \n            f1_scores.append(f1_score.to(\'cpu\').numpy())\n            losses.append(loss.to(\'cpu\').numpy())\n\n        metric = [np.mean(f1_scores), np.mean(losses)]   \n        \n    return metric\n'})}),"\n",(0,t.jsx)(e.h2,{id:"training_loop",children:"training_loop()"}),"\n",(0,t.jsx)(e.p,{children:"Function to train the neural network through backward propagation."}),"\n",(0,t.jsx)(e.h3,{id:"params-1",children:"Params"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"train_loader:"})," DataLoader with the training dataset."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"val_loader:"})," DataLoader with the validation dataset."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"learning_rate:"})," Initial learning rate for training the network."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"starter_channels:"})," Starting number of channels in th U-Net"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"momentum:"})," Momentum used during training."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"number_epochs:"})," Number of training epochs."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"loss_function:"})," Function to calculate loss."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"accu_function:"})," Function to calculate accuracy (Default: BinaryF1Score)."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Love:"})," Boolean to decide between training with LoveDA dataset or our own dataset."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"decay:"})," Factor in which learning rate decays."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"bilinear:"})," Boolean to decide the upscaling method (If True Bilinear if False Transpose convolution. Default: True)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"n_channels:"})," Number of initial channels (Defalut 4 [Planet])"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"n_classes:"})," Number of classes that will be predicted (Default 2 [Binary segmentation])"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"plot:"})," Boolean to decide if training loop should be plotted or not."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"seed:"})," Seed that will be used for generation of random values."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"best_model:"})," f1-score of the best model trained. (Calculated on validation dataset)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"model_saved:"})," The best model trained."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"spearman:"})," Spearman correlation calculated for training progress (High positive value will indicate positive learning)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import numpy as np\nimport torch\nfrom collections import deque\n\nfrom utils import LOVE_resample_fly, get_training_device\n"})}),"\n",(0,t.jsx)(e.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"def training_loop(network, train_loader, val_loader, learning_rate, momentum, number_epochs, loss_function, accu_function = BinaryF1Score(), Love = False, binary_love = False, decay = 0.75, bilinear = True, n_channels = 4, n_classes = 2, plot = True, seed = 8):\n    \"\"\"\n        Function to train the Neural Network.\n\n        Input:\n            - train_loader: DataLoader with the training dataset.\n            - val_loader: DataLoader with the validation dataset.\n            - learning_rate: Initial learning rate for training the network.\n            - starter_channels: Starting number of channels in th U-Net\n            - momentum: Momentum used during training.\n            - number_epochs: Number of training epochs.\n            - loss_function: Function to calculate loss.\n            - accu_function: Function to calculate accuracy (Default: BinaryF1Score).\n            - Love: Boolean to decide between training with LoveDA dataset or our own dataset.\n            - decay: Factor in which learning rate decays.\n            - bilinear: Boolean to decide the upscaling method (If True Bilinear if False Transpose convolution. Default: True)\n            - n_channels: Number of initial channels (Defalut 4 [Planet])\n            - n_classes: Number of classes that will be predicted (Default 2 [Binary segmentation])\n            - plot: Boolean to decide if training loop should be plotted or not.\n            - seed: Seed that will be used for generation of random values.\n\n        Output:\n            - best_model: f1-score of the best model trained. (Calculated on validation dataset) \n            - model_saved: The best model trained.\n            - spearman: Spearman correlation calculated for training progress (High positive value will indicate positive learning)\n    \"\"\"\n    \n    device = get_training_device()\n\n    np.random.seed(seed) \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    network = network\n    network.to(device)\n    optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum = momentum, weight_decay=1e-4)\n    \n    #Training metrics are computed as a running average of the last x samples\n    loss_train = deque(maxlen=len(train_loader))\n    accuracy_train = deque(maxlen=len(train_loader))\n\n    val_eps = []\n    val_f1s = []\n    val_loss = []\n\n    train_eps = []\n    train_f1s = []\n    train_loss = []\n    \n    for epoch in tqdm(range(number_epochs), desc = 'Training model'):\n    \n        #Validation phase 0:\n        metric_val = evaluate(network, val_loader, loss_function, accu_function, Love, binary_love)\n\n        val_eps.append(epoch)\n        val_f1s.append(metric_val[0])\n        val_loss.append(metric_val[1])\n            \n        #Training phase:\n        network.train() #indicate to the network that we enter training mode\n        \n        for i, Data in enumerate(train_loader): # Iterate over the training dataset and do the backward propagation.\n            if Love:\n                inputs = LOVE_resample_fly(Data['image'])\n                GTs = LOVE_resample_fly(Data['mask'])\n                if binary_love:\n                    GTs = (GTs == 6).long()\n            else:\n                inputs = Data[0]\n                GTs = Data[1]\n                \n            inputs = inputs.to(device)\n            GTs = GTs.type(torch.long).squeeze().to(device)\n            \n            #Set the gradients of the model to 0.\n            optimizer.zero_grad()\n            # Get predictions\n            pred = network(inputs)\n\n            if (pred.max(1)[1].shape != GTs.shape):\n                GTs = GTs[None, :, :]\n            \n            loss = loss_function(pred, GTs)\n            \n            accu = accu_function.to(device)\n            accu_ = accu(pred.max(1)[1], GTs)\n\n            loss.backward()\n\n            optimizer.step()\n            \n            loss_train.append(loss.item()/GTs.shape[0])\n            accuracy_train.append(accu_.item())\n\n            train_eps.append(epoch+i/len(train_loader))\n            train_f1s.append(np.mean(accuracy_train))\n            train_loss.append(np.mean(loss_train))\n\n        #Validation phase 1:\n        metric_val = evaluate(network, val_loader, loss_function, accu_function, Love, binary_love)\n        print(epoch+1, metric_val)\n\n        val_eps.append(epoch + 1)\n        val_f1s.append(metric_val[0])\n        val_loss.append(metric_val[1])\n        \n        if epoch == 0:\n            best_model = metric_val[0]\n            torch.save(network, 'BestModel.pt')\n            model_saved = network\n        else:\n            if best_model < metric_val[0]:\n                best_model = metric_val[0]\n                torch.save(network, 'BestModel.pt')\n                model_saved = network\n        \n        if (epoch//10 == epoch/10):\n            #After 4 epochs, reduce the learning rate by a factor \n            optimizer.param_groups[0]['lr'] *= decay\n            \n        if plot:\n            fig, ax = plt.subplots(1,1, figsize = (7,5))\n    \n            ax.plot(train_eps, train_f1s, label = 'Training F1-Score', ls= '--', color = 'r')\n            ax.plot(train_eps, train_loss, label = 'Training Loss', ls = '-', color = 'r')\n    \n            ax.plot(val_eps, val_f1s, label = 'Validation F1-Score', ls = '--', color = 'b')\n            ax.plot(val_eps, val_loss, label = 'Validation Loss', ls = '-', color = 'b')\n            \n            ax.text(val_eps[np.argmax(val_f1s)], np.max(val_f1s), str(np.max(val_f1s)))\n    \n            ax.set_xlabel(\"Epoch\")\n    \n            plt.legend()\n    \n            fig.savefig('TrainingLoop.png', dpi = 200)\n\n            plt.close()\n\n    spearman = stats.spearmanr(val_eps, val_f1s)[0]\n\n    if val_eps[np.argmax(val_f1s)] == 0:\n        no_learning = True\n    else:\n        no_learning = False\n        \n    return best_model, model_saved, spearman, no_learning\n"})}),"\n",(0,t.jsx)(e.h2,{id:"train_3fold_domainonly",children:"train_3fold_DomainOnly()"}),"\n",(0,t.jsx)(e.h3,{id:"params-2",children:"Params"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"domain: String with the prefix of the domain to use for training. (Can be either Tanzania or IvoryCoast)"}),"\n",(0,t.jsx)(e.li,{children:"DS_args: List with all the arguments related to the dataset itself (e.g. batch_size, transforms, normalization and use of vegetation indices)"}),"\n",(0,t.jsx)(e.li,{children:"network_args: List with arguments used for the network creation (n_classes, bilinear, starter channels, up_layer)"}),"\n",(0,t.jsx)(e.li,{children:"training_loop_args: List with all the arguments needed to run the training loop (for more information check training_loop funtion.)"}),"\n",(0,t.jsx)(e.li,{children:"eval_args: List with arguments to evaluate the trained network on the test dataset."}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"outputs-2",children:"Outputs"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Stats:"})," List with the mean f1 score and its standard deviation"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dependencies-used-2",children:"Dependencies used"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import time\nimport torch\nimport numpy as np\n\nfrom Dataset.ReadyToTrain_DS import get_DataLoaders\n"})}),"\n",(0,t.jsx)(e.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'def train_3fold_DomainOnly(domain, DS_args, network_args, training_loop_args, eval_args):\n    """\n        Function to run all Domain Only training for the three folds. \n\n        Input:\n            - domain: String with the prefix of the domain to use for training. (Can be either Tanzania or IvoryCoast)\n            - DS_args: List with all the arguments related to the dataset itself (e.g. batch_size, transforms, normalization and use of vegetation indices)\n            - network_args: List with arguments used for the network creation (n_classes, bilinear, starter channels, up_layer)\n            - training_loop_args: List with all the arguments needed to run the training loop (for more information check training_loop funtion.)\n            - eval_args: List with arguments to evaluate the trained network on the test dataset.\n\n        Output:\n            - Stats: Mean and standard deviation of the validation and test accuracy values for the domain only training on the three folds.\n    """\n\n    folds = 3\n\n    fscore = []\n    \n    # For 3-fold Cross-Validation\n    for i in range(folds):\n        \n        # Build Dataloaders\n        print("Creating dataloaders...")\n        train_loader, val_loader, test_loader = get_DataLoaders(domain+\'Split\'+str(i+1), *DS_args)\n        print("Dataloaders created.\\n")\n        \n        n_channels = next(enumerate(train_loader))[1][0].shape[1] #get band number from actual data\n        n_classes = 2\n        \n        # Define the network\n        network = UNet(n_channels, *network_args)\n        \n        # Train the model\n        print("Starting training...")\n        start = time.time()\n        f1_val, network_trained, spearman, no_L = training_loop(network, train_loader, val_loader, *training_loop_args)\n        print("Network trained. Took ", round(time.time() - start, 0), \'s\\n\')\n\n        if i == 0:\n            best_network = network_trained\n            torch.save(best_network, \'OverallBestModel\'+domain+\'.pt\')\n            best_f1 = f1_val\n        else:\n            if f1_val > best_f1:\n                best_network = network_trained\n                torch.save(best_network, \'OverallBestModel\'+domain+\'.pt\')\n                best_f1 = f1_val\n        \n        # Evaluate the model\n        f1_test, loss_test = evaluate(network_trained, test_loader, *eval_args)\n        \n        print("F1_Validation:", f1_val)\n        print("F1_Test:      ", f1_test)\n    \n        fscore.append([f1_val, f1_test])\n    \n    fscore\n    \n    mean = np.mean(fscore, axis = 0)\n    std = np.std(fscore, axis = 0)\n\n    stats = [mean, std]\n\n    return stats\n'})}),"\n",(0,t.jsx)(e.h2,{id:"train_loveda_domainonly",children:"train_LoveDA_DomainOnly()"}),"\n",(0,t.jsx)(e.p,{children:"Function to train the domain only models for the LoveDA dataset."}),"\n",(0,t.jsx)(e.h3,{id:"params-3",children:"Params"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"domain: List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'."}),"\n",(0,t.jsx)(e.li,{children:"DS_args: List with all the arguments related to the dataset itself (e.g. batch_size, transforms)"}),"\n",(0,t.jsx)(e.li,{children:"network_args: List with arguments used for the network creation (n_classes, bilinear, starter channels, up_layer)"}),"\n",(0,t.jsx)(e.li,{children:"training_loop_args: List with all the arguments needed to run the training loop (for more information check training_loop funtion.)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"outputs-3",children:"Outputs"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"validation_accuracy: Accuracy score for validation dataset."}),"\n",(0,t.jsx)(e.li,{children:"network_trained: Neural network that has been trained"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dependencies-used-3",children:"Dependencies used"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"from Dataset.ReadyToTrain_DS import get_LOVE_DataLoaders\n"})}),"\n",(0,t.jsx)(e.h3,{id:"source-code-3",children:"Source code"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"def train_LoveDA_DomainOnly(domain, DS_args, network_args, training_loop_args):\n    \"\"\"\n        Function to train the domain only models for the LoveDA dataset.\n\n        Inputs:\n            - domain: List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'.\n            - DS_args: List with all the arguments related to the dataset itself (e.g. batch_size, transforms)\n            - network_args: List with arguments used for the network creation (n_classes, bilinear, starter channels, up_layer)\n            - training_loop_args: List with all the arguments needed to run the training loop (for more information check training_loop funtion.)\n\n        Outputs:\n            - validation_accuracy: Accuracy score for validation dataset.\n            - network_trained: Neural network that has been trained.\n    \"\"\"\n\n    # Get DataLoaders\n    train_loader, val_loader, test_loader = get_LOVE_DataLoaders(domain, *DS_args)\n\n    # Get number of channels from actual data\n    n_channels = next(enumerate(train_loader))[1]['image'].shape[1] \n\n    # Define the network\n    network = UNet(n_channels, *network_args)\n\n    # Train the network\n    accu_val, network_trained, spearman, no_l = training_loop(network, train_loader, val_loader, *training_loop_args)\n\n    return accu_val, network_trained\n"})}),"\n",(0,t.jsx)(e.h2,{id:"run_domainonly",children:"run_DomainOnly()"}),"\n",(0,t.jsx)(e.p,{children:"Aggregating function to perform the whole training routine for one of the domains."}),"\n",(0,t.jsx)(e.h3,{id:"params-4",children:"Params"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"domain:"})," String with the name of the domain of interest ('Tanzania' or 'IvoryCoast')"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"outputs-4",children:"Outputs"}),"\n",(0,t.jsx)(e.h3,{id:"source-code-4",children:"Source code"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'def run_DomainOnly(domain = \'IvoryCoast\'):\n    """\n        Function to perform the whole training routine for one of the domains.\n    """\n    \n    ## Related to DS\n    batch_size = 4\n    transforms = get_transforms()\n    normalization = \'Linear_1_99\'\n    VI = True\n    DA = False\n    \n    ## Related to the network\n    n_classes = 2\n    bilinear = True\n    starter_channels = 16\n    up_layer = 4\n    attention = True\n    resunet = False\n    \n    ## Related to training and evaluation\n    number_epochs = 30\n    learning_rate = 1\n    momentum = 0.2\n    loss_function = FocalLoss(gamma = 2)\n    accu_function = BinaryF1Score()\n    device = get_training_device()\n    \n    DS_args = [batch_size, transforms, normalization, VI, DA, None, None]\n    network_args = [n_classes, bilinear, starter_channels, up_layer, attention, resunet]\n    training_args = [learning_rate, momentum, number_epochs, loss_function]\n    eval_args = [loss_function, accu_function]\n    \n    Stats = train_3fold_DomainOnly(domain, DS_args, network_args, training_args, eval_args)\n\n    print(Stats)\n    \n    plot_3fold_accuracies(domain, Stats)\n'})})]})}function u(n={}){const{wrapper:e}={...(0,r.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},1151:(n,e,a)=>{a.d(e,{Z:()=>s,a:()=>o});var t=a(7294);const r={},i=t.createContext(r);function o(n){const e=t.useContext(i);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);