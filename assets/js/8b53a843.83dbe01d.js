"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[19],{8396:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>c});var i=s(5893),t=s(1151);const l={},r=void 0,a={id:"Models/BuildingBlocks",title:"BuildingBlocks",description:"Brief description of the submodule",source:"@site/docs/Models/BuildingBlocks.md",sourceDirName:"Models",slug:"/Models/BuildingBlocks",permalink:"/CashewDA-docs/docs/Models/BuildingBlocks",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Models/BuildingBlocks.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Models",permalink:"/CashewDA-docs/docs/category/models"},next:{title:"Loss_Functions",permalink:"/CashewDA-docs/docs/Models/Loss_Functions"}},o={},c=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"DoubleConv",id:"doubleconv",level:2},{value:"Attributes",id:"attributes",level:3},{value:"Methods",id:"methods",level:3},{value:"Source code",id:"source-code",level:3},{value:"Down",id:"down",level:2},{value:"Attributes",id:"attributes-1",level:3},{value:"Methods",id:"methods-1",level:3},{value:"Source code",id:"source-code-1",level:3},{value:"Up",id:"up",level:2},{value:"Attributes",id:"attributes-2",level:3},{value:"Methods",id:"methods-2",level:3},{value:"Source code",id:"source-code-2",level:3},{value:"OutConv",id:"outconv",level:2},{value:"Attributes",id:"attributes-3",level:3},{value:"Methods",id:"methods-3",level:3},{value:"Source code",id:"source-code-3",level:3},{value:"OutDisc",id:"outdisc",level:2},{value:"Attributes",id:"attributes-4",level:3},{value:"Methods",id:"methods-4",level:3},{value:"Source code",id:"source-code-4",level:3},{value:"GradReverse",id:"gradreverse",level:2},{value:"Attributes",id:"attributes-5",level:3},{value:"Methods",id:"methods-5",level:3},{value:"Source code",id:"source-code-5",level:3},{value:"Attention_block",id:"attention_block",level:2},{value:"Attributes",id:"attributes-6",level:3},{value:"Methods",id:"methods-6",level:3},{value:"Source code",id:"source-code-6",level:3}];function d(e){const n={a:"a",annotation:"annotation",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",msub:"msub",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,i.jsxs)(n.p,{children:["In this submodule all of the classes used to build the neural networks are presented. The resulting networks built can be found in the ",(0,i.jsx)(n.a,{href:"./U_net",children:"U_Net"})," module."]}),"\n",(0,i.jsx)(n.h2,{id:"doubleconv",children:"DoubleConv"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the double convolutions performed on the ",(0,i.jsx)(n.a,{href:"./U_net#unet",children:"U-Net"})," architecture."]}),"\n",(0,i.jsxs)(n.p,{children:["The class inherits the attributes and methods of ",(0,i.jsx)(n.code,{children:"torch.nn.Module"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Every convolution block consists of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.strong,{children:"2D convolution"})," layer with","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"kernel size:"})," 3x3"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"padding:"})," 1"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"bias:"})," False"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.strong,{children:"batch normalization"})," layer"]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.strong,{children:"ReLU"})," activation function"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Moreover, an option to include residual connections in the double convolutions is available by using the attribute ",(0,i.jsx)(n.code,{children:"resunet = True"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"attributes",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.in_channels:"})," (int) Number of channels that the module receives."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.out_channels:"})," (int) Number of channels that will be obtained with the two convolutions performed."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.mid_channels:"})," (int) If specified, this will be the number of channels calculated by the first convolution. ",(0,i.jsx)(n.em,{children:"Default"})," is None."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.resunet:"})," (Boolean) Boolean used to indicate if the double convolution will have a residual connection or not. ",(0,i.jsx)(n.em,{children:"Default"})," is False."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.double_conv"})," (torch.nn.Sequential) Sequential layer comprising the convolutions, batch normalizations and ReLU activation functions."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.shortcut:"})," (torch.nn.Conv2d) PyTorch layer with one 2D convolution which will be used as the identity mapping for the residual connection if the ",(0,i.jsx)(n.code,{children:"self.resunet = True"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"kernel size:"})," 1"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"padding:"})," 0"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"stride:"})," 1"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class DoubleConv(nn.Module):\n    """(convolution => [BN] => ReLU) * 2"""\n\n    def __init__(self, in_channels, out_channels, mid_channels=None, resunet = False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.mid_channels = mid_channels\n        self.resunet = resunet\n        \n        if not mid_channels:\n            mid_channels = out_channels\n            \n        self.double_conv = nn.Sequential(\n                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(mid_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n        \n        if resunet:\n            # Identity mapping\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 0, stride = 1, bias = False)\n\n    def forward(self, x):\n        y = self.double_conv(x)\n\n        if self.resunet:\n            s = self.shortcut(x)\n            y = y + s\n        return y\n'})}),"\n",(0,i.jsx)(n.h2,{id:"down",children:"Down"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the downsampling steps used on the contracting path of the ",(0,i.jsx)(n.a,{href:"./U_net#unet",children:"UNet"})," architecture. The downscaling is performed using MaxPooling method with a value of ",(0,i.jsx)(n.code,{children:"pooling = 2"})]}),"\n",(0,i.jsxs)(n.p,{children:["After downsampling with maxpool, a ",(0,i.jsx)(n.a,{href:"#doubleconv",children:"Double Convolution"})," is applied in the lower resolution."]}),"\n",(0,i.jsxs)(n.p,{children:["It inherits the attributes and methods of ",(0,i.jsx)(n.code,{children:"torch.nn.Module"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"attributes-1",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.in_channels:"})," (int) Number of channels that the module receives."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.out_channels:"})," (int) Number of channels that will be obtained with the two convolutions performed."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.resunet:"})," (Boolean) Boolean used to indicate if the double convolution will have a residual connection or not. ",(0,i.jsx)(n.em,{children:"Default"})," is False."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.maxpool_conv:"})," (torch.nn.Sequential) Sequential layer comprising the 2D maxpooling and the posterior double convolutions performed."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-1",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class Down(nn.Module):\n    """Downscaling with maxpool then double conv"""\n\n    def __init__(self, in_channels, out_channels, resunet = False):\n        super().__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.resunet = resunet\n        \n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels, resunet = resunet)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"up",children:"Up"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the upsampling steps performed on the expanding path of the ",(0,i.jsx)(n.a,{href:"./U_net#unet",children:"UNet"})," architecture."]}),"\n",(0,i.jsxs)(n.p,{children:["The upsampling can be performed either using bilinear interpolation or transpose convolutions. After the upsampling is done, the feature map of the same level in the contracting path is concatenated and then a ",(0,i.jsx)(n.a,{href:"#doubleconv",children:"Double convolution"})," is performed."]}),"\n",(0,i.jsxs)(n.p,{children:["Moreover, an option of including attention gates is possible using the attribute ",(0,i.jsx)(n.code,{children:"self.attention = True"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["This class inherits the attributes and methods of ",(0,i.jsx)(n.code,{children:"torch.nn.Module"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"attributes-2",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.in_channels:"})," (int) Number of channels that the module receives."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.out_channels:"})," (int) Number of channels that will be obtained with the two convolutions performed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.bilinear:"})," (Boolean) Boolean to indicate if the upsampling method will be either bilinear interpolation or transpose 2D convolutions. ",(0,i.jsx)(n.em,{children:"Default"})," is False."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.attention:"})," (Boolean) Boolean to indicate if attention gates will be added on the upsampling step. ",(0,i.jsx)(n.em,{children:"Default"})," is False."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.resunet:"})," (Boolean) Boolean used to indicate if the double convolution will have a residual connection or not. ",(0,i.jsx)(n.em,{children:"Default"})," is False."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-2",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class Up(nn.Module):\n    """Upsampling then double conv"""\n\n    def __init__(self, in_channels, out_channels, bilinear=True, attention = False, resunet = False):\n        super().__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.resunet = resunet\n        self.bilinear = bilinear\n        self.attention = attention\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, resunet = resunet)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels, resunet = resunet)\n        \n        if attention:\n            self.attn = Attention_block(in_channels//2, in_channels//2, in_channels//4)\n\n    def forward(self, x1, x2):\n        \n        a1 = self.up(x1)\n        \n        diffY = x2.size()[2] - a1.size()[2]\n        diffX = x2.size()[3] - a1.size()[3]\n        \n        a1 = F.pad(a1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        \n        if self.attention:\n            x2 = self.attn(g=a1,x=x2)\n\n        x = torch.cat([x2, a1], dim=1)\n        \n        return self.conv(x)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"outconv",children:"OutConv"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the final convolutional layer used for segmenting the final image in the ",(0,i.jsx)(n.a,{href:"./U_net#unet",children:"U-Net"})," and the ",(0,i.jsx)(n.a,{href:"./U_net#unetdann",children:"U-Net + DANN"})," architectures."]}),"\n",(0,i.jsx)(n.p,{children:"It gives as an output the logits, with which, the classes predicted can be obtained."}),"\n",(0,i.jsx)(n.h3,{id:"attributes-3",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.in_channels:"})," (int) Number of channels that the module receives."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.out_channels:"})," (int) Number of channels that will be obtained with the convolution performed. This should be the number of classes that will be predicted in the image."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-3",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-3",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class OutConv(nn.Module):\n    """\n        Final convolutional layer used for segmentation purposes.\n    """\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.in_channels =  in_channels\n        self.out_channels =    out_channels\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n'})}),"\n",(0,i.jsx)(n.h2,{id:"outdisc",children:"OutDisc"}),"\n",(0,i.jsx)(n.p,{children:"Class containing the fully connected layers used to discriminate if the features extracted for an image come from the source domain or the target domain."}),"\n",(0,i.jsx)(n.p,{children:"The fully connected layer is composed of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.strong,{children:"flattening layer"})," to convert the feature maps into a unidimensional tensor."]}),"\n",(0,i.jsxs)(n.li,{children:["Three ",(0,i.jsx)(n.strong,{children:"Linear"})," fully connected layers."]}),"\n",(0,i.jsxs)(n.li,{children:["Two ",(0,i.jsx)(n.strong,{children:"ReLU"})," activation functions after the first two fully connected layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"attributes-4",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.in_feat:"})," (int) Number of features that enter the layer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"self.mid_layers:"})," (int) Number of layers"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-4",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-4",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class OutDisc(nn.Module):\n    def __init__(self, in_feat, mid_layers):\n        super(OutDisc, self).__init__()\n        self.disc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=in_feat, out_features=mid_layers, bias = False),\n            nn.ReLU(),\n            nn.Linear(in_features = mid_layers, out_features = mid_layers//2, bias = False),\n            nn.ReLU(),\n            nn.Linear(in_features = mid_layers//2, out_features = 1, bias = False)\n        )\n\n    def forward(self, x):\n        return self.disc(x)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"gradreverse",children:"GradReverse"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the function to perform the reversal of the gradient in the discriminator part of the ",(0,i.jsx)(n.a,{href:"./U_net#unetdann",children:"U-Net + DANN"})," architecture."]}),"\n",(0,i.jsxs)(n.p,{children:["It inherits all of the attributes and methods from ",(0,i.jsx)(n.code,{children:"torch.autograd.Function"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The function applied to the discriminator gradient during the backward propagation is:"}),"\n",(0,i.jsx)(n.span,{className:"katex-display",children:(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{mathvariant:"normal",children:"\u2207"}),(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{children:"d"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"c"}),(0,i.jsx)(n.mi,{children:"r"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"m"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"n"}),(0,i.jsx)(n.mi,{children:"a"}),(0,i.jsx)(n.mi,{children:"t"}),(0,i.jsx)(n.mi,{children:"o"}),(0,i.jsx)(n.mi,{children:"r"})]})]}),(0,i.jsx)(n.mo,{children:"="}),(0,i.jsx)(n.mo,{children:"\u2212"}),(0,i.jsx)(n.mi,{children:"\u03bb"}),(0,i.jsx)(n.mo,{children:"\u22c5"}),(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{mathvariant:"normal",children:"\u2207"}),(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{children:"d"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"c"}),(0,i.jsx)(n.mi,{children:"r"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"m"}),(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mi,{children:"n"}),(0,i.jsx)(n.mi,{children:"a"}),(0,i.jsx)(n.mi,{children:"t"}),(0,i.jsx)(n.mi,{children:"o"}),(0,i.jsx)(n.mi,{children:"r"})]})]})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\nabla_{discriminator} = -\\lambda \\cdot \\nabla_{discriminator} "})]})})}),(0,i.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord",children:"\u2207"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(n.span,{className:"vlist-r",children:[(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(n.span,{className:"mord mtight",children:[(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"d"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"i"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"scr"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"imina"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"or"})]})})]})}),(0,i.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(n.span,{})})})]})})]}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(n.span,{className:"mrel",children:"="}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.7778em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(n.span,{className:"mord",children:"\u2212"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"\u03bb"}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(n.span,{className:"mbin",children:"\u22c5"}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord",children:"\u2207"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(n.span,{className:"vlist-r",children:[(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(n.span,{className:"mord mtight",children:[(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"d"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"i"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"scr"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"imina"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"or"})]})})]})}),(0,i.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(n.span,{})})})]})})]})]})]})]})}),"\n",(0,i.jsxs)(n.p,{children:["The value of ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"\u03bb"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\lambda"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"\u03bb"})]})})]})," will determine the weight of learning of the discriminator head, during the ",(0,i.jsx)(n.strong,{children:"Domain Adaptation"})," phase."]}),"\n",(0,i.jsxs)(n.p,{children:["Big shout out to ",(0,i.jsx)(n.a,{href:"https://github.com/CuthbertCai/pytorch_DANN/tree/master",children:"CuthbertCai"})," who had already implemented the gradient reversal function in PyTorch, which I used for my implementation of the ",(0,i.jsx)(n.a,{href:"./U_Net#unetdann",children:"U-Net + DANN"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"attributes-5",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"constant:"})," (Float) Number indicating the weight of learning done by the discriminator head in ",(0,i.jsx)(n.a,{href:"./U_Net#unetdann",children:"U-Net + DANN"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-5",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"backward:"})," Function to perform the backward propagation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"grad_reverse:"})," Function to apply the gradient reversal to a PyTorch layer."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-5",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class GradReverse(torch.autograd.Function):\n    """\n    Extension of gradient reversal layer\n    """\n    @staticmethod\n    def forward(ctx, x, constant):\n        ctx.constant = constant\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_output = grad_output.neg() * ctx.constant\n        return grad_output, None\n\n    def grad_reverse(x, constant):\n        return GradReverse.apply(x, constant)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"attention_block",children:"Attention_block"}),"\n",(0,i.jsxs)(n.p,{children:["Class containing the implementation of the ",(0,i.jsx)(n.a,{href:"https://paperswithcode.com/method/attention-gate",children:"attention gates"})," used in the attention U-Net. Atention gates help the network focus on targeted regions by supressing feature activations in regions that are not that important."]}),"\n",(0,i.jsxs)(n.p,{children:["Big shout out to ",(0,i.jsx)(n.a,{href:"https://github.com/LeeJunHyun/Image_Segmentation/blob/master/network.py",children:"LeeJunHyun"})," whose implementation of the attention gates inspired the ones used in my implementation."]}),"\n",(0,i.jsx)(n.h3,{id:"attributes-6",children:"Attributes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.F_g:"})," (int) Number of feature maps entering from the gated signal."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.F_l:"})," (int) Number of feature maps entering from the input layer."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.F_int:"})," (int) Number of feature maps resulting after The convolution on the gated signal ",(0,i.jsx)(n.strong,{children:"W_g"})," and the convolution on the input layer ",(0,i.jsx)(n.strong,{children:"W_x"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.W_g"})," (torch.nn.Sequential) Sequential layer with one convolution and one batch normalization layer that will be applied to the gated signal."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.W_x"})," (torch.nn.Sequential) Sequential layer with one convolution and one batch normalization that will be applied to the input layer."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.psi"})," (torch.nn.Sequential) Sequential layer with one convolution, one batch normalization and one sigmoid activation function applied to the result of the addition of the resulting feature maps coming from the gated signal and the input layer."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"self.relu"})," (torch.nn.ReLU) ReLU activation function."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"methods-6",children:"Methods"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"forward:"})," Function to perform the forward step of the group of PyTorch layers."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"source-code-6",children:"Source code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"\nclass Attention_block(nn.Module):\n    def __init__(self,F_g,F_l,F_int):\n        \n        super(Attention_block,self).__init__()\n        \n        self.F_g = F_g\n        self.F_l = F_l\n        self.F_int = F_int\n        \n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n            )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self,g,x):\n        \n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1+x1)\n        psi = self.psi(psi)\n\n        return x*psi\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},1151:(e,n,s)=>{s.d(n,{Z:()=>a,a:()=>r});var i=s(7294);const t={},l=i.createContext(t);function r(e){const n=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);