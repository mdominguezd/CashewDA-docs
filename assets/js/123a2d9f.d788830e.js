"use strict";(self.webpackChunkcashew_da_docs=self.webpackChunkcashew_da_docs||[]).push([[888],{784:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>l,default:()=>o,frontMatter:()=>t,metadata:()=>r,toc:()=>m});var a=n(5893),i=n(1151);const t={},l=void 0,r={id:"Dataset/ReadyToTrain_DS",title:"ReadyToTrain_DS",description:"Brief description of the submodule",source:"@site/docs/Dataset/ReadyToTrain_DS.md",sourceDirName:"Dataset",slug:"/Dataset/ReadyToTrain_DS",permalink:"/CashewDA-docs/docs/Dataset/ReadyToTrain_DS",draft:!1,unlisted:!1,editUrl:"https://github.com/${organizationName}/${projectName}/tree/main/docs/Dataset/ReadyToTrain_DS.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DatasetSplit",permalink:"/CashewDA-docs/docs/Dataset/DatasetSplit"},next:{title:"Transforms",permalink:"/CashewDA-docs/docs/Dataset/Transforms"}},d={},m=[{value:"Brief description of the submodule",id:"brief-description-of-the-submodule",level:2},{value:"calculate_percentiles()",id:"calculate_percentiles",level:2},{value:"Params",id:"params",level:3},{value:"Outputs",id:"outputs",level:3},{value:"Dependencies used",id:"dependencies-used",level:3},{value:"Source code",id:"source-code",level:3},{value:"get_DataLoaders()",id:"get_dataloaders",level:2},{value:"Params",id:"params-1",level:3},{value:"Outputs",id:"outputs-1",level:3},{value:"Dependencies used",id:"dependencies-used-1",level:3},{value:"Source code",id:"source-code-1",level:3},{value:"get_LOVE_DataLoaders()",id:"get_love_dataloaders",level:2},{value:"Size of the dataset:",id:"size-of-the-dataset",level:4},{value:"Params",id:"params-2",level:3},{value:"Outputs",id:"outputs-2",level:3},{value:"Dependencies used",id:"dependencies-used-2",level:3},{value:"Source code",id:"source-code-2",level:3},{value:"Img_Dataset",id:"img_dataset",level:2},{value:"Size of the dataset:",id:"size-of-the-dataset-1",level:4},{value:"Normalization",id:"normalization",level:4},{value:"Vegetation indices",id:"vegetation-indices",level:4},{value:"Attributes",id:"attributes",level:3},{value:"Methods",id:"methods",level:3},{value:"__len__()",id:"__len__",level:4},{value:"plot_imgs()",id:"plot_imgs",level:4},{value:"__getitem__()",id:"__getitem__",level:4},{value:"Source code",id:"source-code-3",level:3}];function c(e){const s={a:"a",annotation:"annotation",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.h2,{id:"brief-description-of-the-submodule",children:"Brief description of the submodule"}),"\n",(0,a.jsx)(s.p,{children:"Here all of the code used to pass from the datasets downloaded to ready to use data for training is described."}),"\n",(0,a.jsx)(s.h2,{id:"calculate_percentiles",children:"calculate_percentiles()"}),"\n",(0,a.jsx)(s.p,{children:"Function to calculate 0.01 and 0.99 percentiles of the bands of planet images. These values will be later used for normalizing the dataset."}),"\n",(0,a.jsx)(s.h3,{id:"params",children:"Params"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"img_folder:"})," ",(0,a.jsx)(s.em,{children:"(list)"})," The name of the folder with the images."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"samples:"})," ",(0,a.jsx)(s.em,{children:"(integer)"})," The number of images to take to calculate these percentiles, for computing reasons not all images are considered."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"outputs",children:"Outputs"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"vals:"})," ",(0,a.jsx)(s.em,{children:"(numpy.ndarray)"})," The mean 1% and 99% quantiles for the images analysed."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"dependencies-used",children:"Dependencies used"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import os\nimport random\nimport numpy as np\nimport rioxarray\n"})}),"\n",(0,a.jsx)(s.h3,{id:"source-code",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'def calculate_percentiles(img_folder, samples = 400):\n    """\n        Function to calculate 0.01 and 0.99 percentiles of the bands of planet images. These values will be later used for normalizing the dataset.\n\n        Inputs:\n            - img_folder: The name of the folder with the images.\n            - samples: The number of images to take to calculate these percentiles, for computing reasons not all images are considered.\n        Output:\n            - vals: The mean 1% and 99% quantiles for the images analysed.\n    """\n    imgs = [fn for fn in os.listdir(img_folder) if \'StudyArea\' in fn]\n\n    random.seed(8)\n    img_sample = random.sample(imgs, samples)\n    quantiles = np.zeros((2,4))\n    \n    for i in img_sample:\n        quantiles += rioxarray.open_rasterio(img_folder + "\\\\" + i).quantile((0.01, 0.99), dim = (\'x\',\'y\')).values\n    \n    vals = quantiles/len(img_sample)\n    \n    return vals\n'})}),"\n",(0,a.jsx)(s.h2,{id:"get_dataloaders",children:"get_DataLoaders()"}),"\n",(0,a.jsxs)(s.p,{children:["Function to get the training, validation and test torch.DataLoader or torch.Dataset for a specific dataset. This function gets the images from the ",(0,a.jsx)(s.a,{href:"./ReadyToTrain_DS#img_dataset",children:"Img_Dataset class"}),"."]}),"\n",(0,a.jsx)(s.h3,{id:"params-1",children:"Params"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"dir:"})," (str) Directory with the name of the data to be used."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"batch_size:"})," (int) Size of the batches used for training."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"transform:"})," (torchvision.transforms.V2.Compose) torch composition of transforms used for image augmentation."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"normaliztion:"})," (str) Type of normalization used. (Should be 'Linear_1_99')"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"VI:"})," (boolean) Boolean indicating if NDVI and NDWI are also used in training."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"split_size:"})," (float) Float between 0 and 1 indicating the fraction of dataset to be used (Especifically useful for HP tuning)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"only_get_DS:"})," (boolean) Boolean for only getting datasets instead of dataloaders."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"train_split_size:"})," (float) fraction of train split to be loaded. (number between 0 and 1)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"val_split_size:"})," (float) fraction of validation and test split to be loaded. (number between 0 and 1)"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"outputs-1",children:"Outputs"}),"\n",(0,a.jsx)(s.p,{children:"Can be either the data loaders:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"train_loader:"})," Training torch data loader"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"val_loader:"})," Validation torch data loader"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"test_loader:"})," Test torch data loader"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"or the datasets:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"train_DS:"})," Training torch data set."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"val_DS:"})," Validation torch data loader"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"test_DS:"})," Test torch data loader"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"dependencies-used-1",children:"Dependencies used"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import torch\nfrom torch.utils.data import random_split\n"})}),"\n",(0,a.jsx)(s.h3,{id:"source-code-1",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'def get_DataLoaders(dir, batch_size, transform, normalization, VI, only_get_DS = False, train_split_size = None, val_split_size = None):\n    """\n        Function to get the training, validation and test data loader for a specific dataset.\n\n        Inputs:\n            - dir: Directory with the name of the data to be used.\n            - batch_size: Size of the batches used for training.\n            - transform: torch composition of transforms used for image augmentation.\n            - normaliztion: Type of normalization used.\n            - VI: Boolean indicating if NDVI and NDWI are also used in training.\n            - split_size: Float between 0 and 1 indicating the fraction of dataset to be used (Especifically useful for HP tuning)\n            - only_get_DS: Boolean for only getting datasets instead of dataloaders.\n        Output:\n            - train_loader: Training torch data loader\n            - val_loader: Validation torch data loader\n            - test_loader: Test torch data loader\n    """\n    \n    train_DS = Img_Dataset(dir, transform, norm = normalization, VI=VI)\n    val_DS = Img_Dataset(dir, split = \'Validation\', norm = normalization, VI=VI)\n    test_DS = Img_Dataset(dir, split = \'Test\', norm = normalization, VI=VI)\n\n    if train_split_size != None:\n        if val_split_size == None:\n            val_split_size = train_split_size\n            \n        train_DS, l = random_split(train_DS, [train_split_size, 1-train_split_size], generator=torch.Generator().manual_seed(8))\n        val_DS, l = random_split(val_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        test_DS, l = random_split(test_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        \n    train_loader = torch.utils.data.DataLoader(dataset=train_DS, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_DS, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(dataset=test_DS, batch_size=batch_size, shuffle=False)\n    \n    if only_get_DS:\n        return train_DS, val_DS, test_DS\n    else:\n        return train_loader, val_loader, test_loader\n'})}),"\n",(0,a.jsx)(s.h2,{id:"get_love_dataloaders",children:"get_LOVE_DataLoaders()"}),"\n",(0,a.jsxs)(s.p,{children:["Function to get the loaders for LoveDA dataset, which was retrieved using ",(0,a.jsx)(s.a,{href:"https://torchgeo.readthedocs.io/en/stable/api/datasets.html#loveda",children:"torchgeo"}),"."]}),"\n",(0,a.jsx)(s.h4,{id:"size-of-the-dataset",children:"Size of the dataset:"}),"\n",(0,a.jsxs)(s.table,{children:[(0,a.jsx)(s.thead,{children:(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Domain"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Train"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Validation"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Test"})})]})}),(0,a.jsxs)(s.tbody,{children:[(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"Urban"})}),(0,a.jsx)(s.td,{children:"1,155"}),(0,a.jsx)(s.td,{children:"677"}),(0,a.jsx)(s.td,{children:"820"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"Rural"})}),(0,a.jsx)(s.td,{children:"1,366"}),(0,a.jsx)(s.td,{children:"992"}),(0,a.jsx)(s.td,{children:"976"})]})]})]}),"\n",(0,a.jsx)(s.h3,{id:"params-2",children:"Params"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"domain:"})," List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"batch_size:"})," Number of images per batch."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"transforms:"})," Image augmentations that will be considered."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"only_get_DS:"})," Boolean for only getting datasets instead of dataloaders."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"train_split_size:"})," Amount of images from training split that will be considered. (Float between 0 and 1)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"val_split_size:"})," Amount of images from validation and test split that will be considered. (Float between 0 and 1)"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"outputs-2",children:"Outputs"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"train_loader:"})," Training torch LoveDA data loader"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"val_loader:"})," Validation torch LoveDA data loader"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"test_loader:"})," Test torch LoveDA data loader"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"dependencies-used-2",children:"Dependencies used"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import torch\nfrom torchgeo.datasets import LoveDA\nfrom torch.utils.data import random_split\n"})}),"\n",(0,a.jsx)(s.h3,{id:"source-code-2",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"def get_LOVE_DataLoaders(domain = ['urban', 'rural'], batch_size = 4, transforms = None, only_get_DS = False, train_split_size = None, val_split_size = None):\n    \"\"\"\n        Function to get the loaders for LoveDA dataset.\n\n        Inputs:\n            - domain: List with the scene parameter for the LoveDa dataset. It can include 'rural' and/or 'urban'.\n            - batch_size: Number of images per batch.\n            - transforms: Image augmentations that will be considered.\n            - train_split_size: Amount of images from training split that will be considered. (Float between 0 and 1)\n            - val_split_size: Amount of images from validation split that will be considered. (Float between 0 and 1)\n            - only_get_DS: Boolean for only getting datasets instead of dataloaders.\n\n        Output:\n            - train_loader: Training torch LoveDA data loader\n            - val_loader: Validation torch LoveDA data loader\n            - test_loader: Test torch LoveDA data loader\n    \"\"\"\n    if transforms != None:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True, transforms = transforms)\n    else:\n        train_DS = LoveDA('LoveDA', split = 'train', scene = domain, download = True)\n        \n    test_DS = LoveDA('LoveDA', split = 'test', scene = domain, download = True, transforms = transforms)\n    val_DS = LoveDA('LoveDA', split = 'val', scene = domain, download = True, transforms = transforms)\n\n    if train_split_size != None:\n        if val_split_size == None:\n            val_split_size = train_split_size\n        train_DS, l = random_split(train_DS, [train_split_size, 1-train_split_size], generator=torch.Generator().manual_seed(8))\n        val_DS, l = random_split(val_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n        test_DS, l = random_split(test_DS, [val_split_size, 1-val_split_size], generator=torch.Generator().manual_seed(8))\n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_DS, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_DS, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(dataset=test_DS, batch_size=batch_size, shuffle=False)\n\n    if only_get_DS:\n        return train_DS, val_DS, test_DS\n    else:\n        return train_loader, val_loader, test_loader\n"})}),"\n",(0,a.jsx)(s.h2,{id:"img_dataset",children:"Img_Dataset"}),"\n",(0,a.jsxs)(s.p,{children:["Class to manage the ",(0,a.jsx)(s.strong,{children:"Cashew dataset"}),". The Cashew dataset consists of 256x256 ",(0,a.jsx)(s.a,{href:"https://www.planet.com/nicfi/?gad_source=1&gclid=CjwKCAiAk9itBhASEiwA1my_67JOFQ8L4DPicJ47w-b_bGBjLBM1SymMjL91UsJVmB5jSRwKsoedZxoCb2sQAvD_BwE",children:"Planet NICFI"})," images with 4 bands(B, G, R, NIR)."]}),"\n",(0,a.jsx)(s.h4,{id:"size-of-the-dataset-1",children:"Size of the dataset:"}),"\n",(0,a.jsxs)(s.table,{children:[(0,a.jsx)(s.thead,{children:(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"fold"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Domain"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Train"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Validation"})}),(0,a.jsx)(s.th,{children:(0,a.jsx)(s.strong,{children:"Test"})})]})}),(0,a.jsxs)(s.tbody,{children:[(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"1"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"IvoryCoast"})}),(0,a.jsx)(s.td,{children:"8225"}),(0,a.jsx)(s.td,{children:"411"}),(0,a.jsx)(s.td,{children:"38"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"1"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"Tanzania"})}),(0,a.jsx)(s.td,{children:"1021"}),(0,a.jsx)(s.td,{children:"57"}),(0,a.jsx)(s.td,{children:"31"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"2"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"IvoryCoast"})}),(0,a.jsx)(s.td,{children:"7770"}),(0,a.jsx)(s.td,{children:"214"}),(0,a.jsx)(s.td,{children:"337"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"2"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"Tanzania"})}),(0,a.jsx)(s.td,{children:"1142"}),(0,a.jsx)(s.td,{children:"49"}),(0,a.jsx)(s.td,{children:"29"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"3"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"IvoryCoast"})}),(0,a.jsx)(s.td,{children:"9267"}),(0,a.jsx)(s.td,{children:"466"}),(0,a.jsx)(s.td,{children:"120"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"3"})}),(0,a.jsx)(s.td,{children:(0,a.jsx)(s.strong,{children:"Tanzania"})}),(0,a.jsx)(s.td,{children:"1142"}),(0,a.jsx)(s.td,{children:"40"}),(0,a.jsx)(s.td,{children:"28"})]})]})]}),"\n",(0,a.jsx)(s.h4,{id:"normalization",children:"Normalization"}),"\n",(0,a.jsxs)(s.p,{children:["The normalization of the images in the dataset was performed using a linear normalization using the values of the percentiles of 1 and 99 percent. A nice explanation of image normalization can be found on this ",(0,a.jsx)(s.a,{href:"https://medium.com/sentinel-hub/how-to-normalize-satellite-images-for-deep-learning-d5b668c885af",children:"medium post"}),"."]}),"\n",(0,a.jsx)(s.p,{children:"The equation used for the normalization is presented below:"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"M"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"G"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"n"}),(0,a.jsx)(s.mi,{children:"o"}),(0,a.jsx)(s.mi,{children:"r"}),(0,a.jsx)(s.mi,{children:"m"}),(0,a.jsx)(s.mi,{children:"a"}),(0,a.jsx)(s.mi,{children:"l"}),(0,a.jsx)(s.mi,{children:"i"}),(0,a.jsx)(s.mi,{children:"z"}),(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"d"})]})]}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsxs)(s.mfrac,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"M"}),(0,a.jsx)(s.mi,{children:"G"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"P"}),(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"r"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"c"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"%"})]})]})]}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"P"}),(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"r"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"c"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"99"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"%"})]})]}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"P"}),(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"r"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"c"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"%"})]})]})]})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"IMG_{normalized} = \\frac{IMG - Perc_{1\\%}}{Perc_{99\\%} - Perc_{1\\%}}"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"G"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3361em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"or"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"ma"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.01968em"},children:"l"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"ze"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"d"})]})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1.4562em",verticalAlign:"-0.5064em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,a.jsx)(s.span,{className:"mfrac",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.9498em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.655em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"P"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"er"}),(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"c"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3448em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.3448em",marginLeft:"0em",marginRight:"0.0714em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.5357em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size3 size1 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"99%"})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.2306em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"P"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"er"}),(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"c"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3448em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.3448em",marginLeft:"0em",marginRight:"0.0714em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.5357em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size3 size1 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"1%"})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.2306em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})})]}),(0,a.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,a.jsxs)(s.span,{style:{top:"-3.4714em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"MG"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"P"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02778em"},children:"er"}),(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"c"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3448em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.3448em",marginLeft:"0em",marginRight:"0.0714em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.5357em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size3 size1 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"1%"})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.2306em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.5064em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose nulldelimiter"})]})]})]})]})}),"\n",(0,a.jsx)(s.h4,{id:"vegetation-indices",children:"Vegetation indices"}),"\n",(0,a.jsxs)(s.p,{children:["Two additional channels can be added to the tensor which are the Normalized Difference Vegetation Index ",(0,a.jsx)(s.strong,{children:"(NDVI)"})]}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"D"}),(0,a.jsx)(s.mi,{children:"V"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsxs)(s.mfrac,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"R"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"R"})]}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"R"}),(0,a.jsx)(s.mo,{children:"+"}),(0,a.jsx)(s.mi,{children:"R"})]})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"NDVI = \\frac{NIR - R}{NIR + R}"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1.2757em",verticalAlign:"-0.4033em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,a.jsx)(s.span,{className:"mfrac",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.8723em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.655em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"+"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"})]})})]}),(0,a.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,a.jsxs)(s.span,{style:{top:"-3.394em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.4033em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose nulldelimiter"})]})]})]})]})}),"\n",(0,a.jsxs)(s.p,{children:["and the Normalized Difference Water Index ",(0,a.jsx)(s.strong,{children:"(NDWI)"}),"."]}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"D"}),(0,a.jsx)(s.mi,{children:"W"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsxs)(s.mfrac,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"G"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"R"})]}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"G"}),(0,a.jsx)(s.mo,{children:"+"}),(0,a.jsx)(s.mi,{children:"N"}),(0,a.jsx)(s.mi,{children:"I"}),(0,a.jsx)(s.mi,{children:"R"})]})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"NDWI = \\frac{G - NIR}{G+NIR}"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"W"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1.2757em",verticalAlign:"-0.4033em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,a.jsx)(s.span,{className:"mfrac",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.8723em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.655em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"G"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"+"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"})]})})]}),(0,a.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,a.jsxs)(s.span,{style:{top:"-3.394em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"G"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.00773em"},children:"R"})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.4033em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose nulldelimiter"})]})]})]})]})}),"\n",(0,a.jsx)(s.h3,{id:"attributes",children:"Attributes"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"self.img_folder"})," (str) Name of the folder in which the images are stored."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"self.transform"})," (torchvision.transforms.V2.Compose) torch composition of transforms used for image augmentation."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"self.split"})," (str) Split of the dataset to be retrieved. Can be Train, Validation or Test."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"self.norm"})," (str) Type of normalization used. Only 'Linear_1_99' is allowed right now."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"self.VI"})," (boolean) Boolean indicating if NDVI and NDWI are also used in training."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"methods",children:"Methods"}),"\n",(0,a.jsx)(s.h4,{id:"__len__",children:"__len__()"}),"\n",(0,a.jsx)(s.p,{children:"Method to calculate the number of images in the dataset."}),"\n",(0,a.jsx)(s.h4,{id:"plot_imgs",children:"plot_imgs()"}),"\n",(0,a.jsxs)(s.p,{children:["Method to plot a specific image of the dataset. Receives ",(0,a.jsx)(s.strong,{children:"idx"})," as the index of the image and ",(0,a.jsx)(s.strong,{children:"VIs"})," as a boolean to decide to plot or not the vegetation indices of the images."]}),"\n",(0,a.jsx)(s.h4,{id:"__getitem__",children:"__getitem__()"}),"\n",(0,a.jsx)(s.p,{children:"Method to get the tensors (image and ground truth) for a specific index (idx)."}),"\n",(0,a.jsx)(s.h3,{id:"source-code-3",children:"Source code"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"class Img_Dataset(Dataset):\n    \"\"\"\n        Class to manage the cashew dataset.\n    \"\"\"\n    def __init__(self, img_folder, transform = None, split = 'Train', norm = 'Linear_1_99', VI = True, recalculate_perc = False):\n        self.img_folder = img_folder\n        self.transform = transform\n        self.split = split\n        self.norm = norm\n        self.VI = VI\n\n        # Depending of the domain the images will have different attributes (country and quantiles)\n        if 'Tanzania'  in self.img_folder:\n            self.country = 'Tanzania'\n            \n            if recalculate_perc:\n                self.quant_TNZ = calculate_percentiles(img_folder)\n            else:\n                self.quant_TNZ = quant_TNZ\n        else:\n            self.country = 'IvoryCoast'\n            \n            if recalculate_perc:\n                self.quant_CIV = calculate_percentiles(img_folder)\n            else:\n                self.quant_CIV = quant_CIV\n\n    def __len__(self):\n        \"\"\"\n            Method to calculate the number of images in the dataset.    \n        \"\"\"\n        return sum([self.split in i for i in os.listdir(self.img_folder)])//2\n\n    def plot_imgs(self, idx, VIs = False):\n        \"\"\"\n            Method to plot a specific image of the dataset.\n            \n            Input:\n                - self: The dataset class and its attributes.\n                - idx: index of the image that will be plotted.\n                - VIs: Boolean describing if vegetation indices should be plotted\n        \"\"\"\n\n        im, g = self.__getitem__(idx)\n\n        if VIs:\n            fig, ax = plt.subplots(2,2,figsize = (12,12))\n\n            ax[0,0].imshow(im[[2,1,0],:,:].permute(1,2,0))\n            ax[0,0].set_title('Planet image')\n            ax[0,1].imshow(g[0,:,:])\n            ax[0,1].set_title('Cashew crops GT')\n\n            VIs = im[4:6]\n\n            g1=ax[1,0].imshow(VIs[0], cmap = plt.cm.get_cmap('RdYlGn', 5), vmin = 0, vmax = 1)\n            ax[1,0].set_title('NDVI')\n            fig.colorbar(g1)\n            g2=ax[1,1].imshow(VIs[1], cmap = plt.cm.get_cmap('Blues_r', 5), vmin = 0, vmax = 1)\n            ax[1,1].set_title('NDWI')\n            fig.colorbar(g2)\n\n        else:\n            fig, ax = plt.subplots(1,2,figsize = (12,6))\n\n            ax[0].imshow(im[[2,1,0],:,:].permute(1,2,0))\n            ax[0].set_title('Planet image')\n            ax[1].imshow(g[0,:,:])\n            ax[1].set_title('Cashew crops GT')\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n            Method to get the tensors (image and ground truth) for a specific image.\n        \"\"\"\n    \n        conversion = T.ToTensor()\n\n        img = io.imread(fname = self.img_folder + '/Cropped' + self.country + self.split + 'StudyArea_{:05d}'.format(idx) + '.tif').astype(np.float32)\n\n        if self.VI:\n            if self.norm == 'Linear_1_99':\n                ndvi = (img[:,:,3] - img[:,:,2])/(img[:,:,3] + img[:,:,2]) \n                ndwi = (img[:,:,1] - img[:,:,3])/(img[:,:,3] + img[:,:,1])\n\n        if self.norm == 'Linear_1_99':\n            for i in range(img.shape[-1]):\n                if 'Tanz' in self.img_folder:\n                    img[:,:,i] = (img[:,:,i] - self.quant_TNZ[0,i])/(self.quant_TNZ[1,i] - self.quant_TNZ[0,i])\n                elif 'Ivor' in self.img_folder:\n                    img[:,:,i] = (img[:,:,i] - self.quant_CIV[0,i])/(self.quant_CIV[1,i] - self.quant_CIV[0,i])\n\n        if self.VI:\n            ndvi = np.expand_dims(ndvi, axis = 2)\n            ndwi = np.expand_dims(ndwi, axis = 2)\n            img = np.concatenate((img, ndvi, ndwi), axis = 2)\n\n        img = conversion(img).float()\n\n        img = torchvision.tv_tensors.Image(img)\n\n        GT = io.imread(fname = self.img_folder + '/Cropped' + self.country + self.split + 'GT_{:05d}'.format(idx) + '.tif').astype(np.float32)\n\n        GT = torch.flip(conversion(GT), dims = (1,))\n\n        GT = torchvision.tv_tensors.Image(GT)\n\n        if self.transform != None:\n            GT, img = self.transform(GT, img)\n\n        return img, GT\n"})})]})}function o(e={}){const{wrapper:s}={...(0,i.a)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},1151:(e,s,n)=>{n.d(s,{Z:()=>r,a:()=>l});var a=n(7294);const i={},t=a.createContext(i);function l(e){const s=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),a.createElement(t.Provider,{value:s},e.children)}}}]);